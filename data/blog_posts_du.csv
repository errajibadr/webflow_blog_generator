Name;Slug;Collection ID;Locale ID;Item ID;Created On;Updated On;Published On;Titre;auteur;Date de publication;Durée de lecture;Top article;Top 3articles;Derniers articles;Table des matières;Contenu article;articles similaires;Type d'article;Type d'article 2;Type d'article 3;Type d'article 4;type article 5;Type d'article 6;Type d'article 7;Type d'article 8;Photo auteur;photo article;Résumé de l'article;Balise title;META DESCRIPTION
"""Deploy & Scale"" en toute simplicité ";deploy-scale-en-toute-simplicite;66ddd0fdfa9ffe66693562eb;66cf477ebc1fcf6548c64d6b;66fc1b5e3bc96fa3b7312319;Tue Oct 01 2024 15:55:10 GMT+0000 (Coordinated Universal Time);Sun Oct 06 2024 16:40:17 GMT+0000 (Coordinated Universal Time);Sun Oct 06 2024 16:46:06 GMT+0000 (Coordinated Universal Time);"L'avènement du ""Deploy & Scale"" en toute simplicité ";Erraji Badr;Mon Sep 30 2024 00:00:00 GMT+0000 (Coordinated Universal Time);10min;false;true;true;"<h6 id=""""><a href=""#titre_1"" id="""">I - <strong id="""">ECS | ELB | Auto-scaling : la solution de facilité :</strong></a></h6><p id="""">‍</p><h6 id=""""><a href=""#titre_2"">II - <strong id="""">l’architecture Deploy &amp; Scale complète</strong></a></h6><p id="""">‍</p><h6 id=""""><a href=""#titre_3"">III - <strong id=""""> La super star - Infra as a Code - CloudFormation :</strong></a></h6><p id="""">‍</p>";"<p id="""">Le monde avance à grande vitesse. Survivre revient à se mettre au rythme et les projets techniques ne sont pas épargnés. </p><p id="""">Historiquement, la mise en place de l'infrastructure nécessaire pour les applications web exigeait un investissement considérable en temps, en ressources et en expertise.</p><p id="""">De l'achat du matériel à la configuration des équilibreurs de charge, en passant par la garantie d'une haute disponibilité, le processus était complexe, chronophage et semé d’embûches.</p><p id="""">Aujourd’hui, le <strong id="""">cloud</strong> <strong id="""">computing</strong> propose une solution alternative… <strong id="""">AWS, Azure ou GCP &nbsp;</strong>permettent de créer des solutions complexes à l’aide de services gérés et automatisés qui non seulement simplifient le déploiement mais permettent de gérer une haute disponibilité et tolérance aux pannes sans gros casses-têtes ( Ce dernier étant l’expertise propre des <strong id="""">cloud providers</strong>).</p><p id="""">Aujourd'hui, un seul développeur peut non seulement coder sa web-app ou son API, mais aussi, avec un peu de connaissances en AWS, provisionner toute une infrastructure en quelques dizaines de lignes de code. Cette infrastructure peut gérer l'accessibilité, la scalabilité, la sécurité, et même l'intégration continue, transformant ainsi des tâches autrefois complexes et chronophages en un processus rapide, efficace, et entièrement automatisé.<br><br>‍</p><h2 id="""">‍<br><strong id="""">L’Intégration avec AWS :</strong></h2><p id="""">‍</p><div data-rt-embed-type='true'><a name=""titre_1""></a></div><h3 id=""""><strong id="""">I - &nbsp;ECS/ELB/Autoscaling : la solution de facilité :</strong></h3><h3 id=""""><strong id=""""> </strong></h3><h3 id=""""><strong id="""">A - ECS , la solution packagée : </strong></h3><p id=""""><strong id="""">ECS</strong> est idéal pour les applications conteneurisées complexes qui nécessitent une gestion fine des ressources, un contrôle précis des environnements d'exécution, ou des processus de longue durée. Par exemple, si vous avez une application microservices avec plusieurs composants qui doivent communiquer entre eux, ECS est un choix naturel.</p><p id="""">‍<br></p><h3 id=""""><strong id="""">B - ELB, le répartiteur de charge :</strong></h3><p id="""">L'<strong id="""">Elastic Load Balancer (ELB)</strong> est un service qui s'assure que le trafic des utilisateurs est réparti équitablement entre les différentes instances de ton application. Si une instance est occupée ou rencontre un problème, ELB redirige automatiquement le trafic vers une autre instance disponible. Cela garantit que ton application reste accessible et performante, même en cas de forte demande ou de panne d'un serveur.</p><p id="""">‍</p><h3 id=""""><strong id="""">C - Autoscaling, l'adaptabilité automatique :</strong></h3><p id="""">Le <strong id="""">service d'Auto Scaling</strong> permet à ton application de s'adapter automatiquement aux variations de la demande. Si le trafic augmente, Auto Scaling ajoutera des instances pour gérer la charge. Inversement, si le trafic diminue, il réduira le nombre d'instances pour économiser des ressources. Cela garantit que ton application utilise toujours la bonne quantité de ressources, ni plus ni moins, tout en restant disponible pour les utilisateurs.</p><p id="""">‍</p><p id="""">‍</p><div data-rt-embed-type='true'><a name=""titre_2""></a></div><h2 id=""""><strong id="""">II - l’architecture complète </strong></h2><p id="""">‍</p><p id="""">L’architecture est complexe et fait intervenir plusieurs concepts différents comme vous pouvez le voir sur le schéma ci-dessous.</p><p id="""">Cependant avec AWS <strong id="""">cloudformation, </strong>toute l’infra peut être provisionnée en quelques lignes de code. Nous verrons dans la 3ème partie la Stack responsable de la création de cette infra : </p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66fc1b5e3bc96fa3b73122cc_66fc1b52e3387adbc1ffa9bf_ecs_stack_explained.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Voyons comment tous ces services intéragissent entre eux :</p><p id="""">‍</p><h4 id=""""><strong id="""">L'image Docker : Ta principale responsabilité</strong></h4><ul id=""""><li id=""""><strong id="""">Task Definition</strong> : Ton travail principal en tant que développeur sera de créer une image Docker pour ton application. Cette image contient tout ce dont ton application a besoin pour fonctionner (le code, les dépendances, etc.).</li><li id="""">Tu définis ensuite une <strong id="""">Task Definition</strong> qui décrit comment cette image Docker doit être exécutée sur AWS (combien de CPU et de mémoire sont nécessaires, quelles commandes doivent être exécutées, etc.).</li></ul><h4 id=""""><strong id="""">ECS Cluster : Le chef d'orchestre</strong></h4><ul id=""""><li id="""">Une fois que ton image Docker est prête, elle sera exécutée dans ce qu'on appelle un <strong id="""">ECS Cluster</strong>. Le cluster est un ensemble de ressources informatiques (des serveurs EC2) qui exécuteront ton application.</li><li id="""">Le <strong id="""">ECS Service</strong> dans le cluster veille à ce que ton application reste en cours d'exécution et puisse répondre à toutes les requêtes des utilisateurs.</li></ul><h4 id=""""><strong id="""">Auto Scaling : Adapte automatiquement les ressources</strong></h4><ul id=""""><li id="""">Pour s'assurer que ton application puisse gérer plus de trafic lorsqu'elle devient populaire, un <strong id="""">Auto Scaling Group</strong> gère le nombre de serveurs EC2 (machines virtuelles) en fonction de la demande. Il peut augmenter ou réduire le nombre de serveurs automatiquement.</li><li id="""">Le <strong id="""">Capacity Provider</strong> fait le lien entre l'Auto Scaling Group et ton ECS Cluster pour s'assurer que les ressources sont allouées de manière optimale.</li></ul><h4 id=""""><strong id="""">Load Balancer (ELB) : Distribution du trafic</strong></h4><ul id=""""><li id="""">Un <strong id="""">Elastic Load Balancer (ELB)</strong> reçoit tout le trafic des utilisateurs (les requêtes). Il distribue ensuite ce trafic de manière équitable entre les serveurs de ton ECS Cluster qui exécutent ton application.</li><li id="""">Le <strong id="""">Target Group</strong> est le groupe de serveurs (EC2 instances) où ton application est exécutée. Le Load Balancer s'assure que chaque requête est envoyée à un serveur capable de la gérer.</li></ul><p id="""">‍</p><p id="""">‍</p><div data-rt-embed-type='true'><a name=""titre_1""></a></div><h2 id=""""><strong id="""">III - La super star - Infra as a Code :</strong></h2><p id="""">‍</p><h4 id="""">A- Considérations sur la Stack :</h4><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66fc1b5e3bc96fa3b73122de_66fc1b3ea81113a9286dd3d6_ecs_stack_segr.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Pour avoir un aperçu de la stack globale qui est utilisée pour provisionner l’infrastructure visitée	plus haut dans le diagramme, vous pouvez visiter le GitHub qui contient la template en entier. GitHub repo : <a href=""https://github.com/errajibadr/DataEngineeringUnboxed"" id="""">https://github.com/errajibadr/DataEngineeringUnboxed</a></p><p id="""">Pour des raisons pratiques, il peut être plus intéressant de séparer la Stack en plusieurs simplement pour ne pas avoir à tout redéployer si on veut changer une partie de la Stack.</p><p id="""">Dans le cas d’ECS, une ségrégation intéressante à faire peut être entre :</p><p id="""">	le côté <strong id="""">infra</strong> dur : le Cluster, le cluster Provider, sclaingGroup qui vont plus gérer les machines physiques. &nbsp;</p><p id="""">	le côté <strong id="""">service</strong> &nbsp;: qui lui est à un couche d’abstraction plus haut.</p><p id="""">Ce qui nous donnerait plutôt cela comme découpage : </p><p id="""">Dans le diagramme ci-dessus, les éléments de l'infrastructure de base, tels que le Cluster ECS, le Capacity Provider et le Scaling Group, sont mis en évidence en <strong id="""">orange</strong>. Ces éléments gèrent les aspects matériels de l'infrastructure. Les services exécutés sur cette infrastructure, tels que le Service ECS, le Load Balancer et les groupes cibles, sont mis en évidence en <strong id="""">bleu</strong>, indiquant leur rôle en tant que couche d'abstraction supérieure.</p><p id="""">Bon nous pouvons passer aux choses sérieuses, nous allons présenter ici un à un les éléments importants de la Stack.</p><p id="""">‍<br></p><h4 id="""">B - la Stack </h4><p id="""">Les stack CloudFormation peuvent être composés de plusieurs éléments que nous détaillerons plus bas : &nbsp;</p><p id="""">	- Outputs </p><p id="""">	- parameters</p><p id="""">	- mappings </p><p id="""">	- resources</p><p id="""">Pour voir la Stack complète : rdv ici. <a href=""https://github.com/errajibadr/DataEngineeringUnboxed/aws"" target=""_blank"" id="""">https://github.com/errajibadr/DataEngineeringUnboxed</a></p><p id="""">‍</p><h5 id=""""><strong id="""">1. Mappings (Mappages)</strong></h5><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">But</strong> : Définir comment inclure les configurations réseau spécifiques à différents comptes et régions AWS</li></ul><pre></pre><p id="""">‍</p><p id=""""><strong id="""">Explication</strong> : Cette section importe des configurations réseau externes stockées dans un bucket S3, permettant à la stack de s'adapter dynamiquement aux paramètres spécifiques au compte et à la région.</p><h5 id=""""><strong id="""">2. Outputs (Sorties)</strong></h5><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">But</strong> : Fournir des références aux composants clés de l'infrastructure que d'autres stacks ou services peuvent utilise</li></ul><pre></pre><p id="""">‍</p><h5 id=""""><strong id="""">3. Parameters (Paramètres)</strong></h5><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">But</strong> : Définir des valeurs configurables qui peuvent être fournies lors du lancement de la stack pour personnaliser son comportement.</li></ul><pre></pre><p id=""""><strong id="""">Explication</strong> : Ces paramètres permettent de personnaliser l'infrastructure, par exemple en spécifiant le propriétaire, l'ID de l'équipe, et le bucket S3 pour les scripts, ce qui rend la stack réutilisable dans différents environnements.</p><p id="""">‍</p><h5 id=""""><strong id="""">4. Resources : les ressources AWS</strong></h5><p id="""">‍</p><h6 id=""""><strong id="""">A - &nbsp; Cluster </strong></h6><p id=""""><strong id="""">But</strong> : Définir le cluster ECS où les services seront déployés.</p><pre></pre><p id=""""><strong id="""">Explication</strong> : Cette ressource définit le cluster ECS, spécifiant le nom, les fournisseurs de capacité (Capacity Providers), et les tags pertinents. Ce cluster constitue la base sur laquelle les services ECS seront déployés.</p><p id="""">‍</p><h6 id="""">B- Autoscaling Group : </h6><p id="""">‍<br></p><p id=""""><strong id="""">But</strong> : Gérer la mise à l'échelle des instances EC2 en fonction de la demande, en s'assurant que le bon nombre d'instances est en cours d'exécution.</p><p id="""">‍</p><pre></pre><p id="""">‍</p><p id=""""><strong id="""">Explication</strong> : Cette section définit un groupe de mise à l'échelle automatique (Auto Scaling Group) qui ajuste dynamiquement le nombre d'instances EC2 en fonction de la demande. Il utilise un modèle de lancement (Launch Template) et applique les règles de mise à l'échelle spécifiées.</p><p id="""">‍</p><h4 id=""""><strong id="""">Le mot de la fin </strong><br></h4><p id="""">En résumé, la mise en place d'une infrastructure cloud performante et sécurisée peut être réalisée de manière efficace et rapide, à condition de bien maîtriser les concepts fondamentaux des applications web et de les combiner avec une bonne compréhension des services Cloud.</p><p id=""""> Une fois ces bases acquises, vous serez en mesure de tirer pleinement du Cloud. Cette approche vous ouvrira des portes nouvelles par rapport à votre gestion des projets. Je vous invite donc à vous intéresser au cloud si ce n’est pas déjà le cas.</p><p id="""">‍</p><p id="""">‍</p><p id="""">‍</p>";;AWS;IAAS;API;Scale;;;;;;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66fc1e0667f3f1058b5f9ea6_hero_pic.jpg;Pour déployer une application robuste et évolutive sur AWS avec ECS, ELB et d'autres services en utilisant quelques lignes de configuration CloudFormation, on peut définir les ressources clés telles qu'un cluster ECS, un Application Load Balancer, une définition de tâche et un service ECS. En examinant chacun de ces composants, on comprend comment ils s'intègrent dans une architecture globale pour former une infrastructure cloud complète et performante ...;Deployer et Scaler en tout simplicité sur AWS;Deployez une application robuste et scalable sur AWS avec ECS, ELB, etc en quelques lignes de config Cloudformation
AI Driven Development;ai-driven-development;66ddd0fdfa9ffe66693562eb;66cf477ebc1fcf6548c64d6b;66e89c06e9ca38b2d426577d;Mon Sep 16 2024 20:58:46 GMT+0000 (Coordinated Universal Time);Mon Oct 14 2024 20:52:35 GMT+0000 (Coordinated Universal Time);Thu Oct 17 2024 22:52:13 GMT+0000 (Coordinated Universal Time);Cursor : L'IDE IA-powered pour développeurs. ;Erraji Badr;Fri Aug 02 2024 00:00:00 GMT+0000 (Coordinated Universal Time);5 min;false;true;false;"<p id=""""><a href=""#titre_1""><strong id="""">I. La révolution silencieuse du développement</strong></a></p><p id="""">‍</p><p id=""""><a href=""#titre_2"" id=""""><strong id="""">II. L'IA au cœur du développement</strong></a></p><p id="""">‍</p><p id=""""><a href=""#titre_3"" id=""""><strong id="""">III. Cursor en action : Transformer le workflow de développement</strong></a></p>";"<div data-rt-embed-type='true'><a name=""titre_1""></a></div><p id=""""><strong id="""">La révolution silencieuse du développement</strong></p><p id="""">‍</p><p id="""">""Je ne code officiellement plus rien... et pourtant, je suis dev !""</p><p id="""">‍</p><p id="""">Cette affirmation provocatrice pourrait sembler exagérée, voire absurde pour beaucoup de développeurs. Pourtant, elle reflète une réalité de plus en plus tangible dans le monde du développement logiciel. Une réalité façonnée par l'émergence d'outils révolutionnaires comme Cursor.</p><p id="""">‍</p><p id="""">Imaginez un instant un IDE qui devient partie prenante de votre projet. Un outil qui comprend non seulement votre code, mais aussi le contexte global de votre projet, se transformant en votre meilleur pair programmeur. Un assistant capable de générer du code complexe, de refactoriser intelligemment, et même de déboguer en comprenant les subtilités de votre application.</p><p id="""">‍</p><p id="""">Ce n'est pas de la science-fiction. Maintenant c’est Cursor … mais demain ? </p><p id="""">‍</p><p id="""">Voilà … La messe est dite. Cursor a été conçu pour être utilisé avec de l'IA. Explorons maintenant ce qu'il propose de nouveau et comment il révolutionne notre façon de coder.</p><p id="""">‍</p><p id="""">L'idée n'est pas tant de vendre <strong id="""">Cursor </strong>comme <strong id="""">IDE</strong>&nbsp;mais surtout de comprendre la naissance d'un nouveau paradigme dans le monde du développement . car j'en suis sûr il y'en aura plein pour le concurrencer dans les jours/années à venir. </p><p>‍</p><div data-rt-embed-type='true'><a name=""titre_2""></a></div><p>‍</p><p id=""""><strong id="""">L'IA au cœur du développement</strong></p><p id="""">‍</p><p id="""">Cursor a été conçu pour intégrer l'IA de manière native. Voici ce qui le distingue :</p><p id="""">‍</p><p id=""""><strong id="""">1. IA omniprésente et intuitive</strong></p><ul id=""""><li id=""""><strong id="""">Ctrl+K</strong> : Invoque l'IA pour éditer, générer ou compléter du code instantanément.</li><li id=""""><strong id="""">Chat IA</strong> : Discutez avec une IA qui comprend l'intégralité de votre codebase.</li></ul><p id=""""><strong id="""">2. Autocomplete surpuissant (Copilot++)</strong></p><ul id=""""><li id="""">Suggestions contextuelles intelligentes, anticipant vos besoins.</li><li id="""">Proposition de diffs complets, suggérant des modifications substantielles.</li></ul><p id=""""><strong id="""">3. Intégration automatique de documentation externe</strong></p><ul id=""""><li id="""">Ajoutez simplement des URLs de documentation (ex: docs.langchain.com).</li><li id="""">Cursor s'occupe de scraper et d'embed automatiquement la documentation pour votre confort.</li><li id="""">L'IA intègre ces informations pour une assistance ultra-précise et contextualisée.</li></ul><p id=""""><strong id="""">4. Flexibilité du choix du LLM</strong></p><ul id=""""><li id="""">Choisissez et changez de modèle de langage selon vos besoins.</li><li id="""">Adaptez l'IA à différents types de projets.</li></ul><p id="""">Ces fonctionnalités transforment Cursor en un véritable partenaire de développement, comprenant le contexte global de votre projet et agissant comme votre meilleur pair programmeur.<br></p><div data-rt-embed-type='true'><a name=""titre_3""></a></div><p id=""""><br><strong id="""">Cursor en action : Transformer le workflow de développement</strong></p><p id="""">‍</p><p id="""">Voyons comment Cursor révolutionne concrètement le travail quotidien des développeurs :</p><p id="""">‍</p><p id=""""><strong id="""">1. Prototypage ultra-rapide </strong></p><ul id=""""><li id=""""><strong id="""">Scénario</strong> : <ul id=""""><li id="""">Vous avez une idée de nouvelle fonctionnalité. ... Ou alors</li><li id=""""> Vous voulez taper une commande sur le terminal mais vous ne vous rappelez plus de la syntaxe.&nbsp;Normal ...&nbsp;qui peut vous en vouloir, il y'a d'inombrables outils à utiliser chacun avec sa syntaxe propre.</li></ul></li></ul><ul id=""""><li id=""""><strong id="""">Avec Cursor</strong> : <ul id=""""><li id="""">ctrl + I, on tape notre idée. Et hop … </li></ul></li></ul><p id=""""><strong id="""">‍</strong></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66f12d22729357c15714ccfb_66f12cfd9aa538230b36bad7_composer_sized.gif"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><ul id=""""><li id="""">Ctrl + k, on demande de l'aide. Et hop ...</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66f136c3bad26eb63287ab75_66f12748a6dde7c698ee4af9_edit.gif"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><ul id=""""><li id=""""><strong id="""">Résultat</strong> : Passez de l'idée au code testable en un temps record. </li></ul><p id="""">‍</p><p id="""">‍</p><p id=""""><strong id="""">2. Refactoring intelligent à grande échelle</strong></p><ul id=""""><li id=""""><strong id="""">Scénario</strong> : Votre projet nécessite une restructuration majeure. Ou parfois vous refactorisez une petite partie du code</li><li id=""""><strong id="""">Avec Cursor</strong> : il vous permet une complétion sur plusieurs lignes en vous basant sur votre dernière action.</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-normal"" style=""max-width:600px"" data-rt-type=""image"" data-rt-align=""normal"" data-rt-max-width=""600px""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66f12d22729357c15714ccf8_66f1296897b0ab269e4cb360_gif_refactor.gif"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">‍</p><p id=""""><strong id="""">3. Debugging assisté par IA</strong></p><ul id=""""><li id=""""><strong id="""">Scénario</strong> : Vous êtes confronté à un bug complexe. Rien de plus simple : <br>on va sur le terminal puis Cmd + Shift + L . </li><li id=""""><strong id="""">Avec Cursor</strong> : L'IA analyse le contexte global, suggère des pistes de résolution. Et plus qu’à accepter la suggestion pour que le code soit automatiquement modifié sans avoir à copier coller à tout va.</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2458px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2458px""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89c06e9ca38b2d4265758_66e899982ea7107a1ed97131_debugging.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">Résultat</strong> : Réduisez drastiquement le temps de débogage, même sur des problèmes complexes. <strong id="""">==&gt;</strong> <strong id="""">Cependant dans cet exemple en particulier, le LLM se trompe puisque la doc sur langraph n’est pas incluse.<br><br></strong>J’ai préféré gardé cette erreur pour vous mettre en garde et ne pas laisser faire Cursor sans réfléchir non plus.</li></ul><p id="""">Mais il &nbsp;se révélera bien trop utile pour lui en tenir rigueur.&nbsp; Comme ci-dessous. Cela appuie juste le fait que vous serez plus sollicité sur des problématiques plus complexes plutôt que sur des erreurs ou oublis de syntaxes.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1056px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1056px""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66f125e58b0435a5876ce114_66f125689aa538230b2fab08_debug.gif"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">‍</p><p id="""">‍</p><p id="""">‍</p><p id=""""><strong id="""">4. Intégration fluide de nouvelles technos/Docs : </strong></p><ul id=""""><li id=""""><strong id="""">Scénario</strong> : Vous devez intégrer une nouvelle bibliothèque peu familière mais surtout récente. <br>Le LLM dont l’entrainement est arrêté pour la plupart à un an en arrière ne vous sont d’aucune utilité dans ces cas là.<br></li><li id=""""><strong id="""">Avec Cursor</strong> : vous pouvez scrapper les sites web de <strong id="""">docs</strong> et faire l’embbeding en deux cliques. &nbsp;</li></ul><p id="""">‍<br></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:500px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""500px""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89c06e9ca38b2d426574e_66e89ae5282cc26be39df632_add_docs_1.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Ensuite Vous pourrez voir quand il aura fini de les indexer dans l'onglet ""Docs"" avec un point vert comme ci-dessous.<br></p><pre></pre><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1890px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1890px""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89c06e9ca38b2d4265753_66e89af6d72a0c50e7623aea_added%2520doc.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍<br></p><ul id=""""><li id=""""><strong id="""">Résultat</strong> : vous pouvez simplement mentionner maintenant <strong id="""">@doc</strong> quand vous voulez qu’il vous aide s’en s’inspirant de la doc associée.</li></ul><p id="""">‍</p><p id="""">‍</p><p id=""""><strong id="""">Conclusion:</strong></p><p id="""">‍</p><p id="""">Un des effets pervers d'un aussi gros gain d'efficacité et de productivité est que je n'ai jamais autant codé de ma vie !!</p><p id="""">‍</p>";;Productivité;AI;Software Engineering;Cursor;IDE;Dev;;;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89e9f1e38e61d230448af_replicate-prediction-p7t3ndaey9rm60chzjn9cjtp5w.webp;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed85f8a3e78bc46dead99f_replicate-prediction-2djsjs0j4hrm40cj1yj8fx529g.webp;Cursor est un IDE révolutionnaire qui intègre l'IA de manière native, transformant radicalement le workflow des développeurs. Grâce à ses fonctionnalités avancées comme l'auto-complétion intelligente, l'intégration de documentation externe et la flexibilité du choix du modèle de langage, Cursor agit comme un véritable partenaire de développement. Il permet un prototypage ultra-rapide, un refactoring intelligent à grande échelle, et un débogage assisté par IA, rendant le développement plus efficace et intuitif.;Cursor : L'IDE IA-powered pour développeurs. ;Découvrez le future des IDEs: Cursor, révolutionnaire qui booste votre productivité grâce à l'IA. Codez plus vite, mieux et de façon intuitive !
Data Driven Revolution;data-driven-revolution;66ddd0fdfa9ffe66693562eb;66cf477ebc1fcf6548c64d6b;66ed95eece20deea9bbfd462;Fri Sep 20 2024 15:34:06 GMT+0000 (Coordinated Universal Time);Tue Oct 01 2024 16:35:47 GMT+0000 (Coordinated Universal Time);Sun Oct 06 2024 16:19:32 GMT+0000 (Coordinated Universal Time);la révolution du DATA-DRIVEN;Erraji Badr;Thu Aug 01 2024 00:00:00 GMT+0000 (Coordinated Universal Time);8;false;false;false;"<h6 id=""""><a href=""#titre_1"">I - Qu'est-ce qu'une approche Data-Driven ?</a></h6><h6 id="""">‍</h6><h6 id=""""><a href=""#titre_2"">II - Pourquoi adopter une approche Data-Driven ?</a></h6><p id="""">‍</p><h6 id=""""><a href=""#titre_3"" id="""">III - Comment devenir <strong id="""">Data-driven</strong> en 5 étapes : </a></h6><p id="""">‍</p>";"<p id="""">---</p><p id=""""> </p><p id=""""><strong id="""">Tesla</strong>.</p><p id="""">C’est l’exemple le plus marquant qui me vient à l’esprit lorsqu’on parle de Data Driven.</p><p id="""">Elle tire une grande partie de sa réputation pour son innovation dans la conduite autonome.</p><p id="""">Quel est le rapport entre les deux?</p><p id="""">‍</p><p id="""">Au coeur de ce projet, s’articule la révolution axée autour de la DATA. </p><p id="""">Au delà d’utiliser les données pour des analyses historiques ou prédictive, la marque a su mettre à profit sa vaste flotte de voiture, avec une vision sans précédent.</p><p id="""">En effet, elle a su tirer profit du nouveau pétrole numérique (ndlr, la data ) qu’elle avait entre les mains.</p><p id="""">‍</p><p id="""">Tesla a ainsi collecté et analysé des données réelles sur la conduite pendant plusieurs années. Ce qui lui a permis de raffiner ses algorithmes de machine Learning pour ses fonctionnalités phares <strong id="""">Autopilot </strong>et se placer bien en amont de la concurrence.</p><p id="""">‍</p><p id="""">‍</p><p id="""">Cette révolution marque un tournant dans notre monde…</p><p id="""">‍</p><p id="""">‍</p><p id="""">Notre monde où les données jouent un rôle de plus en plus central, les entreprises qui parviennent à exploiter efficacement leur richesse informationnelle se placent rapidement en tête de la compétition. Le concept de ""<strong id="""">Data-Driven</strong>"" ne se limite plus aux géants du numérique ; il est devenu un impératif stratégique pour toute organisation, quelle que soit sa taille.</p><p id="""">l’IT n’est plus uniquement un <strong id="""">centre de coût </strong>pour les entreprises non-tech. Et en particulier la <strong id="""">DATA</strong>, qui est devenu un produit à part entière dont on peut tirer un effet de levier énorme pour son business.</p><p id="""">‍</p><p id="""">---</p><div data-rt-embed-type='true'><a name=""titre_1""></a></div><h4 id="""">I - Qu'est-ce qu'une approche Data-Driven ?<br></h4><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed9b4b7a5a10467fb23c7e_66ed9b466911e75bfcdb59c9_Capture%2520d%25E2%2580%2599e%25CC%2581cran%25202024-09-20%2520a%25CC%2580%252017.56.42.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Être ""<strong id="""">Data-Driven</strong>"" signifie prendre des décisions basées sur les données plutôt que sur l'intuition ou l'expérience passée. Les entreprises data-driven intègrent les données au cœur de leur stratégie, de leurs opérations et de leur culture d'entreprise. Cela implique non seulement de collecter des données, mais surtout de les analyser, de les interpréter et de les utiliser pour guider les décisions quotidiennes, les stratégies à long terme et l’innovation.</p><div data-rt-embed-type='true'><a name=""titre_2""></a></div><h4 id="""">II - Pourquoi adopter une approche Data-Driven ?</h4><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:720px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""720px""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed9b4b7a5a10467fb23c7b_66ed994082a239804551a09d_datadriven_mckinsey.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div><figcaption id="""">Étude de Mckinsey sur la propension des entreprises data-driver à être leader.https://www.mckinsey.com/capabilities/growth-marketing-and-sales/our-insights/five-facts-how-customer-analytics-boosts-corporate-performance</figcaption></figure><p id="""">Comme nous l’évoquions plus haut, considérer l’IT et la data comme un centre de coût est une erreur à ne plus commettre. Car aujourd’hui sa valeur ajoutée dans le business n’est plus à prouver. Et cela se résume, selon une étude récente, par une sur-performance dans le business par rapport à la compétition. Comme vous pouvez le voir ci-dessous. </p><p id="""">‍</p><p id="""">‍</p><p id=""""><strong id="""">1. Précision des décisions : </strong>Les décisions basées sur les données sont généralement plus précises et fiables. Elles reposent sur des faits objectifs plutôt que sur des suppositions.</p><p id="""">‍</p><p id=""""><strong id="""">2. Amélioration de l'efficacité opérationnelle :</strong> En utilisant les données pour optimiser les processus, les entreprises peuvent identifier les inefficacités, réduire les coûts et améliorer la productivité.</p><p id="""">‍</p><p id=""""><strong id="""">3. Meilleure compréhension du client :</strong> Les données permettent d'obtenir des insights approfondis sur le comportement des clients, leurs préférences et leurs besoins, ce qui conduit à des produits et services mieux adaptés.</p><p id="""">‍</p><p id=""""><strong id="""">4. Innovation et compétitivité : </strong>Les entreprises qui exploitent les données pour innover sont souvent celles qui restent à la pointe de leur secteur. Elles peuvent anticiper les tendances, adapter leurs stratégies et devancer la concurrence.</p><p id="""">‍</p><div data-rt-embed-type='true'><a name=""titre_3""></a></div><h4 id="""">III - Comment devenir <strong id="""">Data-driven</strong> en 5 étapes : </h4><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed9b4b7a5a10467fb23c8b_66ed9ac6d7b0eef5f4b94717_datadriven-comment.webp"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Aujourd’hui il y’a des solutions et des produits pour tous les goûts. Plus besoin d’être l’immense entreprise en tech avec une infra pharaonique pour tirer profit de la DATA.</p><p id="""">Mais avant tout, il faut préciser que simplement disposer de la donnée ne va pas créer de la valeur tout seul. Les données n’ont de valeur que des connaissances et analyses que vous pouvez en tirer. Et donc pour en tirer un maximum, il faut cibler principalement celles qui sont cruciales à votre coeur de métier.</p><p id="""">‍</p><p id=""""><strong id="""">1. Culture de la donnée :</strong></p><p id=""""> &nbsp; La première étape vers une approche data-driven est de développer une culture d'entreprise qui valorise la donnée. Cela signifie encourager tous les employés, du dirigeant au stagiaire, à penser en termes de données. Formez vos équipes pour qu'elles comprennent l'importance des données et sachent comment les utiliser efficacement.</p><p id="""">‍</p><p id="""">2. <strong id="""">Collecte de données pertinentes :</strong></p><p id=""""> &nbsp; Que vous soyez une petite ou une grande entreprise, commencez par collecter les données qui sont réellement utiles à votre activité. Par exemple, une petite entreprise de e-commerce pourrait commencer par analyser les données de vente, les visites sur le site web et les interactions avec les clients. Une grande entreprise pourrait intégrer des systèmes plus complexes, tels que des CRM (Customer Relationship Management) et des ERP (Enterprise Resource Planning) pour centraliser et analyser ses données.</p><p id="""">‍</p><p id="""">3.<strong id=""""> Infrastructure technologique :</strong></p><p id=""""> &nbsp; Investir dans les bonnes technologies est crucial. Pour une petite entreprise, cela peut commencer par des outils de BI (Business Intelligence) simples comme Google Analytics, ou des plateformes d'analyse de données comme Tableau ou Power BI. Les grandes entreprises peuvent déployer des solutions plus robustes comme des entrepôts de données (Data Warehouses), des lacs de données (Data Lakes) ou des plateformes de Big Data comme Hadoop ou AWS Redshift.</p><p id="""">‍</p><p id="""">4. <strong id="""">Analyse et interprétation :</strong></p><p id=""""> &nbsp; Une fois les données collectées, il est essentiel de savoir les analyser et les interpréter. L'analyse des données doit être alignée avec les objectifs de l'entreprise. Par exemple, une entreprise peut utiliser l'analyse prédictive pour anticiper les tendances du marché ou la segmentation des clients pour mieux cibler ses campagnes marketing.</p><p id="""">‍</p><p id="""">5. <strong id="""">Passer à l'action :</strong></p><p id=""""> &nbsp; Les insights tirés des données n'ont de valeur que s'ils sont utilisés pour guider les actions. Cela pourrait signifier ajuster une campagne marketing en temps réel, optimiser la chaîne d'approvisionnement, ou même pivoter la stratégie globale de l'entreprise en fonction des données disponibles.</p><p id="""">‍</p><p id="""">‍</p><h4 id=""""><strong id="""">Conclusion</strong></h4><p id="""">Devenir une entreprise data-driven n'est plus une option, mais une nécessité pour rester compétitif dans le paysage commercial moderne. Que vous soyez une petite start-up ou une grande entreprise, l'intégration des données dans votre stratégie et vos opérations peut transformer la manière dont vous fonctionnez, vous permettant d'innover, de satisfaire vos clients et de maintenir une longueur d'avance sur vos concurrents. </p><p id="""">La transition vers une approche data-driven commence par un changement de culture, l'acquisition des bonnes technologies, et l'analyse rigoureuse des données pertinentes. Et n'oubliez pas, il est important de commencer petit, tester et apprendre, avant de déployer à grande échelle.</p><p id="""">‍</p>";;Data;Data-Driven;Big data;Data analytics;Decision making;;;;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89e9f1e38e61d230448af_replicate-prediction-p7t3ndaey9rm60chzjn9cjtp5w.webp;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed95c44e889856e423c8c5_datadriven_article_hero.jpg;La révolution data-driven transforme les entreprises en leur permettant de prendre des décisions basées sur les données plutôt que sur l'intuition, comme l'illustre l'exemple de Tesla dans le domaine de la conduite autonome. Cet article explique l'importance de l'approche data-driven, ses avantages, et présente un guide en cinq étapes pour aider les entreprises à devenir data-driven, soulignant que c'est désormais une nécessité pour rester compétitif dans le paysage commercial moderne.;la révolution du DATA-DRIVEN. Pour vous aussi!;Pourquoi vous devez vous lancer dans le data-driven dès maintenant! et utiliser ces données pour innover, optimiser et dominer votre marché.
DataEngineers heroes;dataengineers-heroes;66ddd0fdfa9ffe66693562eb;66cf477ebc1fcf6548c64d6b;66ed56965b64309d6f3a15ff;Fri Sep 20 2024 11:03:50 GMT+0000 (Coordinated Universal Time);Thu Oct 17 2024 14:02:48 GMT+0000 (Coordinated Universal Time);Thu Oct 17 2024 22:52:13 GMT+0000 (Coordinated Universal Time);"Parquet ou comment même des data-engineers peuvent devenir des ""sups""";Erraji Badr;Thu Aug 22 2024 00:00:00 GMT+0000 (Coordinated Universal Time);7min;true;false;false;"<h6 id=""""><a href=""#titre_1"" id="""">I&nbsp;- Columnar Storage vs Row-based Storage</a></h6><h6 id="""">‍</h6><h6 id=""""><a href=""#titre_2"">II - Les secrets cachés de Parquet</a></h6>";"<p id="""">Aujourd'hui, les données numériques représentent 3 à 4% des émissions mondiales de gaz à effet de serre.</p><p id="""">‍</p><p id="""">La big data n'est pas étrangère à cela...</p><p id="""">‍</p><p id=""""><strong id="""">4%</strong> représente la même part d'<strong id="""">émissions</strong> que tous les pays d'<strong id="""">Afrique subsaharienne</strong> réunis.</p><p id="""">‍</p><p id="""">Voyons comment, avec juste une ligne de code et 1-2 calories cérébrales brûlées, nous pouvons réduire considérablement cela.</p><p id="""">‍</p><p id="""">Parquet est désormais toujours référencé comme un format colonnaire. Dans chaque documentation ou guide, il est devenu la norme pour les données structurées.</p><p id="""">Le principal conseil que l'on trouve est que pour le big data ou l'analytique, Parquet est le format roi. On nous encourage à l'utiliser pour économiser de l'espace et accélérer la lecture.</p><p id="""">Mais voyons comment une légère compréhension du format pourrait nous aider à gagner encore plus que ce qui est promis en termes de compression et d'économie d'espace.</p><p id="""">‍</p><p id="""">Bien sûr, nous perdrons un peu en temps de traitement. Mais pour les données fréquemment consultées, nous économiserons beaucoup de bande passante. On peut le considérer comme un surcoût à l'écriture mais avec un énorme retour sur investissement !</p><p id="""">‍</p><p id="""">Nous passerons rapidement en revue 1 - Le stockage colonnaire vs le stockage par lignes pour couvrir les bases du stockage de données. Ensuite, dans 2 - Les secrets d'encodage de Parquet qui ne sont pas si secrets (il suffisait de lire la documentation), nous entrerons dans le vif du sujet.</p><p id="""">‍</p><div data-rt-embed-type='true'><a name=""titre_1""></a></div><h2 id="""">I&nbsp;- Stockage colonnaire vs Stockage par lignes</h2><p id="""">‍</p><p id="""">Tout d'abord, nous devons comprendre qu'il existe différents types de stockage dans le domaine des données structurées.</p><p id="""">Chacun a sa propre utilisation qui peut être résumée comme suit :</p><p id="""">‍</p><ul id=""""><li id=""""><strong id=""""><code id="""">Stockage par lignes :</code></strong><code id=""""> Idéal pour les systèmes OLTP (traitement transactionnel en ligne) où un accès rapide à des enregistrements individuels est nécessaire.</code></li><li id=""""><strong id=""""><code id="""">Stockage colonnaire :</code></strong><code id=""""> Idéal pour les systèmes OLAP (traitement analytique en ligne) où des analyses de données à grande échelle sont effectuées, et où seul un sous-ensemble de colonnes est généralement interrogé.</code></li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed5b6949db5827626efc13_66ed5afbdfba72ccfad3f885_storage_types.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div><figcaption id="""">Représentation pour le stockage par ligne ou par colonne</figcaption></figure><p id="""">‍</p><p id=""""><strong id="""">CSV</strong>, <strong id="""">Json</strong> sont des exemples de stockage par lignes. Ou encore <strong id="""">Avro</strong> une version plus optimisé pour le row-based storage mais principalement utilisé pour la sérialisation des données en streaming.</p><p id="""">‍</p><p id=""""><strong id="""">PARQUET</strong> ou <strong id="""">ORC</strong> sont communément connus comme étant colonnaires... mais...POURQUOI PARQUET EST-IL SPÉCIAL ?</p><p id="""">‍</p><p id=""""><strong id="""">PARQUET</strong> est un TYPE <strong id="""">hybride</strong>, combinant le stockage colonnaire et par lignes</p><p id="""">où les données sont divisées en [Groupes de lignes x Colonnes]</p><p id="""">--&gt; Groupe de lignes 1</p><p id=""""> &nbsp; &nbsp;---&gt; Colonne 0...</p><p id=""""> &nbsp;&nbsp; ---&gt; Colonne x</p><p id="""">--&gt; Groupe de lignes 2.</p><p id=""""> &nbsp;&nbsp;&nbsp;...</p><p id="""">‍</p><p id="""">Chaque <strong id="""">GROUPE DE LIGNES </strong>( <strong id="""">Row Group</strong> ) stocke ses propres métadonnées sur les colonnes afin de les utiliser plus tard. En effet, pour chaque colonne d'un groupe de lignes, il y a des métadonnées min/max/count qui seront utilisées dans les prédicats push-down lors des lectures de parquet.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed5b6a49db5827626efc36_66ed5b145b64309d6f3e3a0e_parquet_row_group.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍</p><div data-rt-embed-type='true'><a name=""titre_2""></a></div><h2 id="""">II. Secrets de Parquet</h2><p id="""">‍</p><p id="""">Comme les données sont stockées en colonnes, dans Parquet, elles peuvent être facilement encodées avant l'écriture.</p><p id="""">Résultant en une grande compression des données. Pour illustrer cela rapidement, générons un jeu de données et stockons-le en csv et parquet.</p><p id="""">‍</p><h3 id="""">Étape 1 : Générer un jeu de données d'exemple</h3><p id="""">‍</p><pre></pre><p id="""">‍</p><p id="""">Comparaison de taille entre CSV et Parquet</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed8c19343a2502e35b9742_66ed5bbcfef3aff44ba0c633_Capture%2520d%25E2%2580%2599e%25CC%2581cran%25202024-09-20%2520a%25CC%2580%252013.25.37.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">‍</p><p id="""">‍</p><p id="""">Avant d'écrire df sur le disque, chaque colonne est encodée avec la meilleure technique pour son type.</p><p id="""">‍</p><p id="""">‍</p><h4 id="""">Exemple 1 : Colonne Pays</h4><p id="""">‍</p><p id=""""><strong id="""">Étape I -</strong> le pays est encodé comme un dictionnaire de pays.</p><p id="""">‍</p><p id=""""><strong id="""">Étape II -</strong> L'encodage <strong id="""">RLE + Bit packing</strong> est ensuite appliqué pour maximiser l'encodage et la compression.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed8f54da3d17b071b4f7b5_66ed5bdc6fbefee4ba3f8f2a_RLE_bit_packing.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Qu'est-ce que cela signifie ?</p><p id="""">‍</p><p id="""">Si nous trions par Pays avant l'écriture, nous aurions une séquence triée qui pourrait conduire à un packing massif.</p><p id="""">Si nous prenons l'exemple ci-dessus, nous aurions ce bit packing : </p><p id=""""><strong id="""">[0,2] [1,2] [2,1] [3,3]</strong> - zéro apparaît 2 fois, 1 apparaît 2 fois, etc...</p><p id="""">‍</p><p id="""">Pour 1 000 lignes, nous pourrions les encoder en un nombre très limité d'octets !!!!!</p><p id="""">Par exemple, ce serait encodé en <strong id="""">[0,650] [1,75] [2,150] [3,125]</strong> résultant en une réduction de taille de 99% dans ce cas spécifique.</p><p id="""">‍</p><h4 id="""">Exemple 2 : Colonne ID et Delta byte Packing</h4><p id="""">‍</p><p id=""""><strong id="""">Étape I - </strong>Calculer le delta entre chaque ligne pour la colonne <strong id="""">ID</strong>. </p><p id="""">Supposons que cette dernière est triée et incrémentée de 1 Alors cela conduirait à un <strong id="""">Delta</strong> entre ligne de : </p><p id="""">‍<strong id="""">1, 1, 1, 1, 1 ..... 1</strong></p><p id="""">‍</p><p id=""""><strong id="""">Étape II </strong>- prendre le delta minimum et calculer le delta relatif.</p><p id="""">Le delta minimum est 1 et les deltas relatifs deviennent :</p><p id=""""><strong id="""">0, 0, 0, 0 ......&nbsp;0</strong></p><p id="""">Les données encodées finales sont :</p><p id="""">en-tête : 8 (taille du bloc), 1 (nombre de miniblocs), 5 (nombre de valeurs), 1 (première valeur)</p><p id="""">‍</p><p id="""">nous finissons avec pratiquement aucune donnée en comparaison avec une colonne id contenant des millions de valeurs si stockée en texte brut. tel que CSV.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed8f53da3d17b071b4f6ee_66ed5becfef3aff44ba0f77c_delta_encoding.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">et en pratique, nous n'avons pas grand chose à faire.&nbsp;Parquet s'occupe de tout ...&nbsp;</p><p id="""">‍</p><pre></pre><p id="""">‍</p><p id="""">Si vous voulez allez plus loin je vous invite à explorer le <a href=""https://github.com/errajibadr/DataEngineeringUnboxed/blob/main/parquet/parquet_encoding_secrets.ipynb"" target=""_blank"">notebook</a> que j'ai mis en place.</p><p id="""">‍</p><p id=""""><strong id="""">Conclusion</strong></p><p id="""">‍</p><p id="""">L'utilisation du format Parquet offre des avantages considérables en termes de compression et d'efficacité de stockage des données. En comprenant et en exploitant les mécanismes d'encodage et de compression de Parquet, notamment le stockage colonnaire et les techniques comme le RLE et le Delta encoding, nous pouvons optimiser significativement nos jeux de données. Cette optimisation ne se traduit pas seulement par des économies d'espace de stockage, mais aussi par une réduction de la consommation d'énergie et, par conséquent, de l'empreinte carbone associée au stockage et à la transmission des données.</p><p id=""""> En tant qu'ingénieurs de données, nous avons la responsabilité et l'opportunité d'utiliser ces outils de manière intelligente pour contribuer à la réduction de l'impact environnemental du big data.</p><p id="""">‍</p>";;big Data;parquet;ESG;python;data engineers;data optimisation;;;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89e9f1e38e61d230448af_replicate-prediction-p7t3ndaey9rm60chzjn9cjtp5w.webp;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed571c8f15a4d00ef00657_out-0-7.webp;Le format Parquet, grâce à son stockage hybride colonnaire et ses techniques avancées d'encodage, permet une compression remarquable des données, réduisant significativement l'espace de stockage nécessaire et l'empreinte carbone associée. En optimisant l'utilisation de Parquet, notamment par le tri judicieux des données avant l'écriture, les ingénieurs de données peuvent maximiser ces bénéfices, faisant de la gestion efficace des données un acte concret en faveur de l'environnement.;;
Gestion des Connexions aux Bases de Données en Python en 2024;gestion-des-connexions-aux-bases-de-donnees-en-python-en-2024;66ddd0fdfa9ffe66693562eb;66cf477ebc1fcf6548c64d6b;67196a96d98a1d9d22c49a56;Wed Oct 23 2024 21:28:54 GMT+0000 (Coordinated Universal Time);Sat Oct 26 2024 20:06:54 GMT+0000 (Coordinated Universal Time);Sat Oct 26 2024 20:19:55 GMT+0000 (Coordinated Universal Time);Gestion des Connexions aux Bases de Données en Python : ;Erraji Badr;Tue Jul 16 2024 00:00:00 GMT+0000 (Coordinated Universal Time);10;false;false;false;"<h5 id="""">I - L'importance de l'Asynchrone</h5><h5 id="""">‍</h5><h5 id="""">II - Stratégies de Gestion des Connexions</h5><h5 id="""">‍</h5><h5 id="""">III - Implémentation d'un Pool de Connexions</h5><h5 id="""">‍</h5><h5 id="""">IV - Tests et Mocking</h5>";"<p>La gestion efficace des connexions aux bases de données est un élément crucial dans le développement d'applications Python performantes. Dans cet article, nous explorerons les meilleures pratiques, les pièges courants à éviter, et comment implémenter des tests robustes.</p><p>‍</p><h2 id="""">L'importance de l'Asynchrone</h2><p id="""">Dans le développement moderne d'applications Python, l'utilisation de connexions asynchrones est devenue incontournable. En effet, la programmation asynchrone offre plusieurs avantages majeurs :</p><ul id=""""><li id="""">Amélioration significative des performances</li><li id="""">Meilleure gestion des ressources système</li><li id="""">Capacité à gérer plus de connexions simultanées</li><li id="""">Réduction des temps de latence</li></ul><p id=""""><strong id="""">Point clé</strong> : L'abandon des clients synchrones au profit de solutions asynchrones peut multiplier les performances de votre application par un facteur significatif.</p><h2 id="""">Stratégies de Gestion des Connexions</h2><h3 id="""">1. Approche Basique : Connexion par Requête</h3><p id="""">‍</p><pre></pre><p>‍</p><p id=""""><strong id="""">Avantages</strong> :</p><ul id=""""><li id="""">Simple à mettre en œuvre</li><li id="""">Facile à comprendre</li></ul><p id=""""><strong id="""">Inconvénients</strong> :</p><ul id=""""><li id="""">Coûteux en ressources</li><li id="""">Risque de fuites de connexions</li><li id="""">Performance limitée</li></ul><h3 id="""">2. Approche Avancée : Pool de Connexions</h3><p id="""">Le pool de connexions représente l'approche recommandée pour les applications en production :</p><p>‍</p><pre></pre><p>‍</p><h3 id="""">Points Essentiels pour les Tests</h3><ol id=""""><li id="""">Reproduire fidèlement le comportement asynchrone</li><li id="""">Tester les context managers</li><li id="""">Vérifier la gestion des ressources</li><li id="""">Simuler les scénarios d'erreur</li></ol><h2 id="""">Bonnes Pratiques et Recommendations</h2><p><strong id="""">1 .&nbsp;Utilisez Toujours des Context Managers</strong></p><pre></pre><p>‍</p><p>2 .&nbsp;Configurez les Timeouts</p><pre></pre><p>‍</p><p>3&nbsp;.&nbsp;Gérez les Erreurs de Connexion</p><pre></pre><p>‍</p><h2 id="""">Conclusion</h2><p id="""">La gestion efficace des connexions aux bases de données est fondamentale pour développer des applications Python performantes et fiables. L'utilisation de connexions asynchrones combinée à un pool de connexions offre la meilleure solution pour la plupart des cas d'usage.</p><p>‍</p><p>‍</p><p id=""""><em id="""">Mots-clés : Python, Database Connection, asyncpg, Connection Pool, Database Testing, Async Programming, Database Management, Python Best Practices</em></p><p>‍</p>";;Python;Database;Async Dev;;;;;;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89e9f1e38e61d230448af_replicate-prediction-p7t3ndaey9rm60chzjn9cjtp5w.webp;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/67196da53897a32523cf8c7f_async_illustration.png;Cet article explore en détail les meilleures pratiques pour gérer les connexions aux bases de données en Python. Il met l'accent sur l'importance de la programmation asynchrone, compare les différentes approches de gestion des connexions (basique vs pool), et fournit des exemples concrets de code pour l'implémentation et les tests. Une attention particulière est portée aux performances, à la gestion des ressources et à la prévention des fuites de connexion.;Gestion des Connexions aux Bases de Données en Python en 2024;Découvrez les meilleures pratiques pour la gestion des connexions aux bases de données en Python. Guide complet sur l'asynchrone, les pools de connexions e
Guide complet des formats de table Lakehouse 2024 : Hudi vs Delta Lake vs Iceberg | Comparaison détaillée;guide-complet-des-formats-de-table-lakehouse-2024-hudi-vs-delta-lake-vs-iceberg-comparaison-detaillee;66ddd0fdfa9ffe66693562eb;66cf477ebc1fcf6548c64d6b;67111427ee855a49b2584e4b;Thu Oct 17 2024 13:41:59 GMT+0000 (Coordinated Universal Time);Thu Oct 17 2024 15:11:53 GMT+0000 (Coordinated Universal Time);Thu Oct 17 2024 22:52:13 GMT+0000 (Coordinated Universal Time);Guide complet des formats de table Lakehouse 2024 : Hudi vs Delta Lake vs Iceberg | Comparaison détaillée;Erraji Badr;Wed Oct 16 2024 00:00:00 GMT+0000 (Coordinated Universal Time);12min;false;false;true;"<p><a href=""#title_1"">1.&nbsp;incremental pipelines</a></p><p>‍</p><p><a href=""#title_2"">2.&nbsp;Contrôle de concurrence </a></p><p>‍</p><p><a href=""#title_3"">3.&nbsp;Merge On Read </a></p><p>‍</p><p><a href=""#title_4"">4. Évolution des partitions </a><a href=""#title_5"">‍</a></p><p>‍</p><p><a href=""#title_5"">5.&nbsp;Transactions ACID </a></p><p>‍</p><p><a href=""#title_6"">6. Quelle technologie Lakehouse choisir en 2024 ?</a></p><p id="""">‍</p>";"<h3 id="""">Les trois mousquetaires du Lakehousing:</h3><p id="""">‍</p><p id="""">Dans l'univers en constante évolution du Big Data et de l'analytique avancée, le concept de Lakehouse a émergé comme une solution révolutionnaire, combinant le meilleur des data lakes et des data warehouses. Au cœur de cette révolution se trouvent trois formats de table majeurs : Apache Hudi, Delta Lake et Apache Iceberg. Chacun offre des fonctionnalités ACID (Atomicité, Cohérence, Isolation, Durabilité) essentielles, mais avec des approches et des forces distinctes. Dans ce guide complet, nous allons plonger dans une comparaison détaillée de ces technologies de pointe pour vous aider à faire le choix le plus éclairé pour votre architecture data en 2024.</p><p id="""">‍</p><h2 id="""">Table des matières</h2><ol id=""""><li id="""">incremental pipeline : La nouvelle frontière du traitement des données ( Adieux Lambda &amp; Kappa ) </li><li id="""">Contrôle de concurrence : Gérer les écritures simultanées efficacement</li><li id="""">Merge On Read : Équilibrer performance d'écriture et de lecture</li><li id="""">Évolution des partitions : S'adapter à la croissance des données</li><li id="""">Transactions ACID : La base d'un Lakehouse fiable</li><li id="""">Quelle technologie Lakehouse choisir en 2024 ?</li><li id="""">Conclusion et perspectives d'avenir</li></ol><p id="""">‍</p><div data-rt-embed-type='true'><a href=""title_1""></div><h2 id="""">Pipelines incrémentales : La nouvelle frontière du traitement des données</h2><h3 id="""">Apache Hudi : Pionnier des pipelines incrémentaux</h3><ul id=""""><li id="""">Suivi natif de tous les changements (ajouts, mises à jour, suppressions)</li><li id="""">Exposition des changements sous forme de flux</li><li id="""">Indexes au niveau des enregistrements pour un traitement efficace des changements</li></ul><h3 id="""">Delta Lake : Rattrapage avec le Change Data Feed</h3><ul id=""""><li id="""">Fonctionnalité récemment ouverte au public avec Delta Lake 2.0</li><li id="""">Permet la consommation incrémentale des changements</li></ul><h3 id="""">Apache Iceberg : Limité aux ajouts incrémentaux</h3><ul id=""""><li id="""">Lecture incrémentale disponible</li><li id="""">Ne supporte pas les mises à jour et suppressions incrémentales</li></ul><p id="""">‍</p><p id="""">‍</p><div data-rt-embed-type='true'><a href=""title_2""></div><h2 id="""">Contrôle de concurrence : Gérer les écritures simultanées efficacement</h2><h3 id="""">Apache Hudi : Contrôle de concurrence granulaire</h3><ul id=""""><li id="""">Contrôle optimiste de la concurrence (OCC) au niveau des fichiers</li><li id="""">Optimisé pour les mises à jour et suppressions fréquentes</li><li id="""">Permet des services de table asynchrones même dans des scénarios multi-écrivains</li></ul><h3 id="""">Delta Lake : Évolution du verrouillage JVM</h3><ul id=""""><li id="""">Initialement limité à un seul nœud Apache Spark</li><li id="""">Améliorations récentes pour le support multi-cluster</li></ul><h3 id="""">Apache Iceberg : OCC standard</h3><ul id=""""><li id="""">Supporte le contrôle optimiste de la concurrence</li><li id="""">Performances à évaluer dans des scénarios de mises à jour fréquentes</li></ul><p id="""">‍</p><p id="""">‍</p><div data-rt-embed-type='true'><a href=""title_3""></div><h2 id="""">Merge On Read : Équilibrer performance d'écriture et de lecture</h2><h3 id="""">Apache Hudi : Support complet de Merge On Read (MoR)</h3><ul id=""""><li id="""">Utilise une combinaison de fichiers Parquet et de fichiers journaux Avro</li><li id="""">Permet un équilibre flexible entre performance d'écriture et de lecture</li><li id="""">Idéal pour les workloads de streaming en temps quasi réel</li></ul><h3 id="""">Delta Lake : Approche basée sur les fichiers Delta</h3><ul id=""""><li id="""">Utilise des fichiers Delta pour stocker les modifications récentes</li><li id="""">Fusion lors des opérations de compaction</li></ul><h3 id="""">Apache Iceberg : Focalisation sur Copy On Write (CoW)</h3><ul id=""""><li id="""">Principalement orienté vers le modèle Copy On Write</li><li id="""">Performances d'écriture potentiellement limitées pour les mises à jour fréquentes</li></ul><p id="""">‍</p><p>‍</p><div data-rt-embed-type='true'><a href=""title_4""></div><h2 id="""">Évolution des partitions : S'adapter à la croissance des données</h2><h3 id="""">Apache Hudi : Approche de clustering flexible</h3><ul id=""""><li id="""">Stratégie de clustering sans nécessité de repartitionnement</li><li id="""">Peut fonctionner avec ou sans partitions</li><li id="""">Comparable à la stratégie de micro-partitionnement de Snowflake</li></ul><h3 id="""">Delta Lake : Repartitionnement via réécriture</h3><ul id=""""><li id="""">Permet la modification du schéma de partition</li><li id="""">Nécessite généralement une réécriture des données</li></ul><h3 id="""">Apache Iceberg : Partitionnement caché</h3><ul id=""""><li id="""">Permet l'évolution des partitions sans réécriture complète</li><li id="""">Complexité potentielle avec plusieurs schémas de partitionnement coexistants</li></ul><p id="""">‍</p><p>‍</p><div data-rt-embed-type='true'><a href=""title_5""></div><h2 id="""">Transactions ACID : La base d'un Lakehouse fiable</h2><h3 id="""">Apache Hudi</h3><ul id=""""><li id="""">Support complet des transactions ACID</li><li id="""">Optimisé pour les opérations fréquentes de mise à jour et de suppression</li></ul><h3 id="""">Delta Lake</h3><ul id=""""><li id="""">Transactions ACID natives</li><li id="""">Intégration étroite avec l'écosystème Databricks</li></ul><h3 id="""">Apache Iceberg</h3><ul id=""""><li id="""">Support des transactions ACID</li><li id="""">Forte compatibilité avec divers moteurs de requête</li></ul><p id="""">‍</p><p>‍</p><div data-rt-embed-type='true'><a href=""title_6""></div><h2 id="""">Quelle technologie Lakehouse choisir en 2024 ?</h2><p id="""">Le choix entre Apache Hudi, Delta Lake et Apache Iceberg dépendra de vos besoins spécifiques :</p><ul id=""""><li id=""""><strong id="""">Optez pour Apache Hudi si :</strong><ul id=""""><li id="""">Vous avez besoin de pipelines incrémentales avancées</li><li id="""">Vos workloads nécessitent des mises à jour et suppressions fréquentes</li><li id="""">La flexibilité entre performance d'écriture et de lecture est cruciale</li></ul></li><li id=""""><strong id="""">Choisissez Delta Lake si :</strong><ul id=""""><li id="""">Vous êtes déjà dans l'écosystème Databricks</li><li id="""">Vous recherchez une intégration étroite avec Apache Spark</li><li id="""">La cohérence des transactions ACID est votre priorité absolue</li></ul></li><li id=""""><strong id="""">Préférez Apache Iceberg si :</strong><ul id=""""><li id="""">Vous avez besoin d'une grande flexibilité dans l'évolution du schéma</li><li id="""">L'intégration avec divers outils d'analyse (Dremio, Trino, Athena) est essentielle</li><li id="""">Vous privilégiez la simplicité du modèle Copy On Write</li></ul></li></ul><p id="""">‍</p><p id="""">‍</p>";;Big data;Lakehouse;Datalake;Deltalake;Data Engineering;;;;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89e9f1e38e61d230448af_replicate-prediction-p7t3ndaey9rm60chzjn9cjtp5w.webp;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/671118be6a6f578ca4b058fd_table_formats_logos.png;Découvrez les différences cruciales entre Apache Hudi, Delta Lake et Apache Iceberg pour optimiser votre stratégie Lakehouse en 2024. Analyse approfondie des fonctionnalités ACID, pipelines incrémentaux, contrôle de concurrence et plus encore.;Guide formats tables Lakehouse 2024 : Hudi vs Delta vs iceberg;Découvrez les différences cruciales entre Apache Hudi, Delta Lake et Apache Iceberg pour optimiser votre stratégie Lakehouse en 2024. Analyse approfondie d
LLM Structured outputs;llm-structured-outputs;66ddd0fdfa9ffe66693562eb;66cf477ebc1fcf6548c64d6b;66ed4afc0a3687eea6afce54;Fri Sep 20 2024 10:14:20 GMT+0000 (Coordinated Universal Time);Thu Oct 17 2024 14:03:03 GMT+0000 (Coordinated Universal Time);Thu Oct 17 2024 22:52:13 GMT+0000 (Coordinated Universal Time);LLM Structured Outputs : Pydantic+Instructor ;Erraji Badr;Tue Sep 03 2024 00:00:00 GMT+0000 (Coordinated Universal Time);10min;false;true;true;"<p id=""""><a href=""#titre_1""><strong id="""">I .</strong> <strong id="""">Pourquoi les Structured Outputs sont essentiels ?</strong></a></p><p id="""">‍</p><p id=""""><a href=""#titre_2""><strong id="""">II .</strong> <strong id="""">Comment ont-ils fait ? </strong></a></p><p id="""">‍</p><p id=""""><a href=""#titre_3""><strong id="""">III . &nbsp;Comment s’en sortir dans l’ombre du géant ? Le potentiel des petits modèles</strong></a></p><p id="""">‍</p><p id=""""><a href=""#titre_4"" id=""""><strong id="""">IV. &nbsp;La solution : Instructor + Pydantic</strong></a></p><p id="""">‍</p>";"<p id=""""><strong id="""">O1 est sorti et alors ? Nous pouvons faire pareil sans utiliser openAI </strong></p><p id="""">‍</p><p id="""">Les <strong id="""">Structured Outputs</strong> avec une garanti à 100% sur le schéma de sorti sont là depuis un mois chez Open AI! &nbsp;(https://openai.com/index/introducing-structured-outputs-in-the-api/)</p><p id=""""><br>Ce paradigme change grandement la donne dans le monde du LLM. OpenAi reprend un large pas en avant sur la concurrence de ce côté là.</p><p id="""">On va essayer de comprendre comment ils ont réussi cette prouesse. </p><p id="""">Puis comment nous simples mortels pouvant approcher cette fiabilité avec de simple modèle de 8b quantisés à 4bytes. ( si vous voulez savoir ça sans vous embêter à découvrir le comment d’open-ai, allez directement à la section 3)</p><p>‍</p><div data-rt-embed-type='true'><a name=""titre_1""></a></div><p id=""""><strong id="""">I .</strong> <strong id="""">Pourquoi les Structured Outputs sont essentiels ?</strong></p><p id=""""><strong id=""""> </strong><br></p><p id="""">Le « Chain of Thought » est une technique efficace pour obtenir de bons résultats, car elle permet au modèle de ""réfléchir"" étape par étape.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1083px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1083px""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed4b527483c897468262ec_66ed4b21b86038a2e77c2730_instructor-cot-Yao%2520et%2520el..webp"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Cependant, elle présente une limitation majeure : le modèle doit produire de nombreuses étapes intermédiaires avant d'arriver au résultat final souhaité. </p><p id="""">Cette verbosité peut être coûteuse en termes de temps de calcul et de ressources et nous oblige à obtenir un long paragraphe alors que ce qui nous intéresse est uniquement le résultat final.</p><p id="""">Cela rend l’utilisation des LLMs au mieux <strong id="""">stochastique</strong> au pire <strong id="""">contre-productive.<br></strong></p><p id="""">Pour pallier ce problème tout en conservant les avantages du raisonnement étape par étape, nous pouvons utiliser des<strong id=""""> sorties structurées</strong>. Par exemple, nous pouvons demander au modèle de produire un JSON avec la structure suivante :</p><pre></pre><p id="""">‍</p><p id="""">Cette approche permet d'obtenir un processus de réflexion structuré sans la verbosité excessive du CoT traditionnel.</p><p id="""">‍</p><h4 id="""">Mais voilà … les LLMs se trompent beaucoup.</h4><p id="""">‍</p><p id="""">Allez à la section III si vous voulez voir directement comment vous pouvez implémenter les structured outputs vous même.</p><p id="""">‍</p><div data-rt-embed-type='true'><a name=""titre_2""></a></div><p id=""""><strong id="""">II .</strong> <strong id="""">Comment ont-ils fait ? </strong></p><p id=""""><strong id="""">‍<br></strong></p><p id="""">Les <strong id="""">Structured Outputs</strong> est un mode de sortie du LLM où on obtient un schéma à la sortie.<br>Mais voilà, jusque’à aujourd’hui cette technique n’était pas très fiable. Les LLMs faisant beaucoup d’erreur dans le domaine. Alors comment open ai a fait pour obtenir 100% ?<br><br>Leur méthode se décompose en deux parties principales :</p><ol id=""""><li id=""""><strong id="""">Entraînement initial</strong> : Ils ont d'abord entraîné leur dernier modèle, gpt-4o-2024-08-06, à comprendre et générer avec précision des sorties basées sur des schémas complexes. Malgré l'atteinte d'un score de référence de 93%, le modèle n'était pas suffisamment fiable pour des applications robustes.</li><li id=""""><strong id="""">Constraint Decoding </strong>: Pour atteindre une fiabilité de 100%, OpenAI a adopté une approche d'ingénierie déterministe appelée ""Constraint Decoding"". Cette technique resserre les restrictions de sortie, garantissant une conformité parfaite aux schémas JSON fournis.</li></ol><p id=""""><strong id="""">Comment fonctionne le décodage sous contrainte ?</strong></p><p id="""">‍</p><p id="""">Le <strong id="""">Constraint Decoding </strong>limite les choix du modèle aux seuls tokens qui correspondent au schéma spécifié. Voici comment cela fonctionne :</p><ol id=""""><li id="""">Le schéma JSON est converti en une grammaire hors contexte (CFG - Context-Free Grammar).</li><li id="""">Les tokens autorisés sont mis à jour dynamiquement au fur et à mesure que le modèle génère chaque partie de la sortie.</li><li id="""">Cette approche garantit que chaque token généré s'aligne parfaitement avec les exigences structurelles.</li></ol><p id="""">‍</p><p id=""""><strong id="""">Grammaire hors contexte (CFG)</strong></p><p id="""">Une CFG est utilisée pour définir les règles syntaxiques que les données JSON doivent suivre. Elle spécifie comment les symboles du langage peuvent être combinés pour former des chaînes valides, similaire aux règles grammaticales d'une langue naturelle.</p><p id="""">‍</p><p id=""""><strong id="""">Mise en œuvre et performance</strong></p><ul id=""""><li id="""">Chaque schéma JSON est prétraité en CFG avant le traitement des requêtes.</li><li id="""">Ce prétraitement entraîne une pénalité de latence initiale, mais permet ensuite des sorties rapides et précises.</li><li id="""">Le traitement d'un schéma standard prend généralement moins de 10 secondes lors de la première exécution, tandis que des schémas plus complexes peuvent nécessiter jusqu'à une minute.</li><li id="""">Les schémas traités sont mis en cache pour un accès plus rapide lors des interactions futures.</li></ul><p id="""">‍</p><div data-rt-embed-type='true'><a name=""titre_3""></a></div><p id=""""><strong id="""">III . &nbsp;Comment s’en sortir dans l’ombre du géant ? Le potentiel des petits modèles :</strong></p><p id="""">Maintenant, imaginons qu'un petit modèle open-source de seulement 8 milliards de paramètres puisse rivaliser avec GPT-4 d'OpenAI sur l'automatisation de tâches et de workflows, sans nécessiter de longs et coûteux processus de fine-tuning ou d'optimisation des préférences. Comment serait-ce possible ?</p><p id="""">Cette idée remet en question la croyance selon laquelle seuls les modèles massifs peuvent offrir des performances de pointe. En réalité, avec les bonnes techniques et outils, même des modèles plus petits peuvent accomplir des tâches impressionnantes.</p><p id="""">Que diriez-vous s'il était possible d'atteindre une fiabilité proche de 100% avec un framework simple et élégant, même avec des modèles plus petits ? Examinons quelques cas d'utilisation concrets pour illustrer les défis actuels et comment nous pouvons les surmonter.</p><p id=""""><strong id="""">Cas d'utilisation 1 : Génération de réponses simples</strong></p><p id="""">Commençons par un cas simple où nous demandons au modèle de générer une réponse dans un format JSON spécifique :</p><p id="""">‍</p><pre></pre><p id="""">Ce cas fonctionne généralement bien, mais il peut parfois y avoir des erreurs dans la génération du JSON.</p><pre></pre><p id="""">‍</p><p id="""">‍</p><p id=""""><strong id="""">Cas d'utilisation 2 : Tentative d'injection de prompt</strong></p><p id="""">Que se passe-t-il si un utilisateur tente de modifier le format de sortie ? il fait une injection de prompt pour forcer le LLM à aller à l'encontre du pompt system.</p><p id="""">‍</p><pre></pre><p id="""">‍</p><p id="""">Dans ce cas, le modèle pourrait être confus et générer une réponse non conforme au format demandé.</p><p id="""">‍</p><pre></pre><p id="""">‍</p><p id=""""><strong id="""">Cas d'utilisation 3 : Utilisation d'outils (Tool Calling)</strong></p><p id="""">‍</p><p id="""">Essayons maintenant d'utiliser la fonction de ""tool calling"" pour structurer notre sortie :</p><p id="""">‍</p><pre></pre><p id="""">‍</p><p id="""">Cette approche améliore la structure de la sortie, mais elle n'est pas infaillible, surtout face à des tentatives d'injection.</p><p id="""">‍</p><pre></pre><p id="""">‍</p><div data-rt-embed-type='true'><a name=""titre_4""></a></div><p id=""""><strong id="""">La solution : Instructor + Pydantic</strong></p><p id="""">‍</p><p id="""">Face à ces défis, la combinaison d'Instructor et Pydantic offre une solution robuste. Voici comment cela fonctionne :</p><ol id=""""><li id="""">À chaque appel, on vérifie la conformité de la réponse au schéma attendu.</li><li id="""">Si ce n'est pas correct, on recommence en demandant au LLM de corriger son erreur.</li></ol><p id=""""><strong id="""">Pydantic : Le validateur de données</strong></p><p id="""">Pydantic se charge de la validation des données. Si quelque chose ne va pas, il renvoie un message d'erreur clair et détaillé. Par exemple :</p><p id="""">‍</p><pre></pre><p id="""">‍</p><p id=""""><strong id="""">Instructor : L'enveloppe intelligente</strong></p><p id="""">‍</p><p id="""">Instructor ""patche"" le client pour l'envelopper dans une boucle. Si le schéma reçu n'est pas conforme, il relance automatiquement une demande.</p><p id="""">‍</p><p id=""""><strong id="""">Démonstration de la robustesse</strong></p><p id="""">Voyons maintenant comment cette approche reste robuste face aux défis précédents :<br></p><p id="""">‍</p><pre></pre><p id=""""><br>Même face à une tentative d'injection, le système maintient la structure attendue et ne permet que les valeurs autorisées pour le champ ""type"".</p><p id="""">‍</p><pre></pre><p id="""">‍</p><p id="""">Voilà comment avec un tout petit peu plus de code, on peut s'en sortir sans faire appel aux Goliaths de l'industrie.</p><p id="""">‍</p>";;LLM;Gen-AI;Structured Outputs;GPT, OpenAI;pydantic;instructor;OpenSource LLM models;;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89e9f1e38e61d230448af_replicate-prediction-p7t3ndaey9rm60chzjn9cjtp5w.webp;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66ed53c3e452549b4bfd0daa_llm_pydantic.webp;"Face aux défis de fiabilité des sorties structurées des petits modèles de langage, Pydantic et Instructor offrent une solution robuste. Pydantic agit comme un validateur de données, vérifiant la conformité des réponses au schéma attendu.
Instructor ""patche"" le client LLM, l'enveloppant dans une boucle qui relance automatiquement une demande si le schéma reçu n'est pas conforme. Cette approche permet d'atteindre une fiabilité proche de 100% dans la génération de sorties structurées, même avec des modèles plus petits et face à des tentatives d'injection de prompt";LLM Structured Outputs : Pydantic+Instructor ;Comment Pydantic+Instructor permettent aux petits modèles de langage de générer des sorties structurées fiables, rivalisant avec les géants IA comme openAI
La Voie du Data Engineer : Guide Complet des Compétences Essentielles en 2024;la-voie-du-data-engineer-guide-complet-des-competences-essentielles-en-2024;66ddd0fdfa9ffe66693562eb;66cf477ebc1fcf6548c64d6b;6723af144122853ca8286664;Thu Oct 31 2024 16:23:48 GMT+0000 (Coordinated Universal Time);Thu Oct 31 2024 16:27:16 GMT+0000 (Coordinated Universal Time);Sun Dec 01 2024 12:09:33 GMT+0000 (Coordinated Universal Time);Exceller en tant que Data Engineer en 2025;erraji Badr;Thu Oct 03 2024 00:00:00 GMT+0000 (Coordinated Universal Time);3;false;false;false;<p>Introduction : Le Chemin Exigeant du Data Engineering</p>;"<h2 id="""">Introduction : Le Chemin Exigeant du Data Engineering</h2><p id="""">‍</p><p id="""">Le Data Engineering est bien plus qu'un simple métier - c'est une véritable voie professionnelle qui demande passion, dévouement et expertise technique. Si vous envisagez cette carrière, préparez-vous à un parcours exigeant mais profondément enrichissant.</p><h2 id="""">‍</h2><h2 id="""">Pourquoi le Data Engineering Est Avant Tout du Développement</h2><p id="""">‍</p><p id="""">Contrairement aux idées reçues, un Data Engineer est d'abord et avant tout un développeur. Cette fondation en software engineering est cruciale pour exceller dans ce domaine en constante évolution. La maîtrise du code n'est pas une option, c'est une nécessité.</p><p id="""">‍</p><h2 id="""">Les 7 Compétences Fondamentales du Data Engineer Moderne</h2><p id="""">‍</p><h3 id="""">1. Software Engineering</h3><ul id=""""><li id="""">Maîtrise approfondie de Python et/ou Java</li><li id="""">Expertise en gestion de version avec Git</li><li id="""">Implémentation de pipelines CI/CD robustes</li></ul><h3 id="""">2. Core DATA</h3><ul id=""""><li id="""">Expertise avancée en SQL</li><li id="""">Maîtrise des Systèmes de Gestion de Base de Données Relationnelles (SGBDR)</li><li id="""">Optimisation des requêtes et performances</li></ul><h3 id="""">3. Architecture DATA</h3><ul id=""""><li id="""">Conception de systèmes data scalables</li><li id="""">Implémentation de solutions de streaming</li><li id="""">Gestion du high throughput</li><li id="""">Modélisation de données</li></ul><h3 id="""">4. BIG DATA</h3><ul id=""""><li id="""">Compréhension des paradigmes de calcul distribué</li><li id="""">Utilisation des frameworks Big Data modernes</li><li id="""">Optimisation des traitements à grande échelle</li></ul><h3 id="""">5. Solutions Cloud</h3><ul id=""""><li id="""">Maîtrise des principales plateformes cloud</li><li id="""">Compréhension des architectures cloud-native</li><li id="""">Optimisation des coûts et performances</li></ul><h3 id="""">6. Modern Data Stack</h3><ul id=""""><li id="""">Exploitation des architectures MPP (Massive Parallel Processing)</li><li id="""">Intégration des outils modernes de data engineering</li><li id="""">Optimisation des pipelines de données</li></ul><h3 id="""">7. DevOps</h3><ul id=""""><li id="""">Conteneurisation avec Docker</li><li id="""">Orchestration avec Kubernetes</li><li id="""">Automatisation des déploiements</li><li id="""">Monitoring et observabilité</li></ul><p id="""">‍</p><h2 id="""">L'Excellence dans l'Art du Data Engineering</h2><p id="""">Le véritable art du Data Engineering réside dans la capacité à maîtriser et orchestrer ces différentes compétences. Ce n'est pas seulement une question de connaissances techniques, mais aussi de vision architecturale et de compréhension des besoins métier.</p><h2 id="""">Conclusion et Perspectives</h2><p id="""">Le chemin du Data Engineer est exigeant mais offre des opportunités uniques pour ceux qui osent le suivre. Dans un monde où la données devient toujours plus stratégique, les Data Engineers jouent un rôle crucial dans la transformation digitale des entreprises.</p><h2 id="""">Pour Aller Plus Loin</h2><p id="""">Découvrez mon article plsu détaillé sur ces différents aspects écrit pour Ossia <a href=""https://www.blog-ossiaconseil.com/blogs/the-way-of-the-data-engineer"" id="""">ici</a> </p><p id="""">‍</p><p>keywords : data engineer, data engineering, big data, software engineering, devops, cloud computing, SQL, python, java, architecture data</p>";;data engineer;data engineering;big data;architecture data;;;;;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89e9f1e38e61d230448af_replicate-prediction-p7t3ndaey9rm60chzjn9cjtp5w.webp;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6723afbd2e7805fefe037eec_replicate-prediction-qhawhrmgadrm60cjdm68zythec.webp;Dans l'univers toujours plus complexe de la data, le rôle du Data Engineer s'est transformé en un art véritable, exigeant une maîtrise technique pointue et une vision holistique des systèmes de données. Cet article plonge au cœur des 7 compétences fondamentales qui définissent l'excellence dans ce domaine : du Software Engineering avec Python et Java, à la maîtrise des architectures Big Data, en passant par le DevOps et le Modern Data Stack. Bien plus qu'un simple guide technique, ce parcours détaillé révèle pourquoi le véritable Data Engineer est d'abord un développeur dans l'âme, capable de jongler entre différentes technologies et approches. Que vous soyez débutant ou professionnel expérimenté, découvrez les clés pour forger votre expertise et relever les défis passionnants du Data Engineering moderne.;La Voie du Data Engineer : Guide des Compétence Essentielle 2025;Découvrez les 7 compétences fondamentales du Data Engineer moderne : du Software Engineering au DevOps. Guide complet pour maîtriser l'art du Data Engineer
Structurer son Projet de machine learning;structurer-son-projet-de-machine-learning;66ddd0fdfa9ffe66693562eb;66cf477ebc1fcf6548c64d6b;670950407852ed438cac92f3;Fri Oct 11 2024 16:20:16 GMT+0000 (Coordinated Universal Time);Sun Oct 27 2024 15:45:28 GMT+0000 (Coordinated Universal Time);Sun Dec 01 2024 12:09:33 GMT+0000 (Coordinated Universal Time);Structurer son Projet de machine learning.;Erraji Badr;Wed Jun 19 2024 00:00:00 GMT+0000 (Coordinated Universal Time);5min;false;false;false;;"<p id="""">La technique est indispensable. C’est bien souvent par elle que commence le progrès. Ce n’est qu’après que les chercheurs interviennent en l’intellectualisant; c’est du moins ce que soutient Nicolas Nassim Taleb dans « antifragile » </p><p id="""">Toutefois la technique n’est pas tout. Avoir une structure clair et bien pensée permet d’aller plus loin et plus vite sur le long terme. </p><p id="""">Et en Datascience, on a la chance que CookieCutter s’en soit occuper en concevant une template auquel notre projet devrait s’efforcer de ressembler. Car suivre rigoureusement quelque chose sans se poser des questions n’est jamais une bonne chose; alors n’hésitez pas à prendre vos liberté et adapter la structure à votre cas!</p><p id="""">‍</p><h2 id=""""><br><br><strong id="""">Cookie&nbsp;Cutter - Qu’est-ce que c’est ? </strong></h2><p id="""">Cookie Cutter est un cli qui permet de générer une structure pour les projets data science.</p><p id="""">Il rajoute une grosse part de « sérieux » et donne enfin cet aspect « industrieux » qui manque tant à notre discipline. </p><p id="""">On n’est plus au niveau d’un data-scientist qui crée 2-3 notebooks et de la data dans tous les sens. Avec qui on peut avoir quelques rapports et analyses bien jolis, mais les étapes pour l’obtenir sont ésotériques et brumeux. La reproductibilité du code est jeté aux oublis. Alors que c’est un point essentiel de la <strong id="""">qualité du code</strong>. </p><p id="""">Un code qui permet de retrouver des résultats corrects et reproductibles.</p><p id="""">Cela se fait en 2 commandes : </p><pre></pre><p id="""">‍</p><p id="""">On obtient alors la structure suivante , modulo quelques changement selon les paramètres de votre projet</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6709530cb8bab23b5dd44c77_670952a3a5aa549f1b609783_ccds_all_struct.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><h2 id=""""><strong id="""">Best Practices Machine learning :</strong></h2><h5 id=""""><strong id="""">et Pourquoi Cookie&nbsp;Cutter en est un condensé ? </strong></h5><p id="""">‍</p><h3 id="""">1 . &nbsp;<strong id="""">L’analyse de données doit être vue comme un graphe orienté acyclique</strong> (DAG).</h3><p id="""">Les éléments les plus importants d'une analyse de données de qualité sont la <strong id="""">justesse</strong> et la <strong id="""">reproductibilité</strong>. </p><p id="""">	—&gt; Toute personne doit pouvoir relancer votre analyse en utilisant uniquement votre code et les données brutes pour obtenir les mêmes résultats finaux.</p><p id="""">‍</p><p id="""">Pour cela, &nbsp;il faut considérer votre pipeline d'analyse comme un DAG, où chaque étape est un nœud dans un graphe orienté sans boucle. </p><p id="""">Vous pouvez exécuter le graphe dans un sens pour recréer <strong id="""">n'importe quel résultat d'analyse</strong>, ou le parcourir en sens inverse pour examiner la combinaison de code et de données qui a conduit à ce résultat</p><p id="""">‍</p><h3 id="""">2.<strong id=""""> Les données brutes sont immuables</strong></h3><p id="""">Dans une analyse de données vue comme un graphe orienté acyclique (DAG), les données brutes doivent rester immuables. Vous pouvez les lire et les copier pour créer de nouveaux résultats, mais <strong id="""">ne</strong> les <strong id="""">modifiez jamais</strong> directement.</p><p id=""""><strong id="""">À éviter :</strong></p><ul id=""""><li id="""">Ne modifiez jamais les données brutes, surtout pas manuellement ou dans Excel.</li><li id="""">Ne remplacez pas les données brutes par des versions traitées.</li><li id="""">Ne sauvegardez pas plusieurs versions des données brutes.</li></ul><p id="""">À noter, qu’il faut éviter des mettre sa donnée dans les sources control tels que git. Mettez les plutôt dans un cloud ( S3, Azure Blob storage …) ou dans un serveur on-prem.</p><p id="""">Le folder « <strong id="""">data/» </strong>devrait aller directement dans le <strong id="""">.gitignore</strong></p><p id="""">‍</p><h3 id=""""><strong id="""">3. Les notebooks sont pour l'exploration et la communication, le code source est pour la répétition</strong></h3><p id="""">Les notebooks, comme Jupyter Notebook ou Apache Zeppelin, sont excellents pour l'exploration des données et la visualisation rapide des résultats. Cependant, ils sont moins adaptés pour reproduire une analyse. Le code source est supérieur pour la répétabilité car il est plus portable, plus facile à tester et à relire.</p><p id="""">‍</p><h3 id=""""><strong id="""">4. Refactorisez les bonnes parties dans du code source</strong></h3><p id="""">Évitez de répéter le même code dans plusieurs notebooks. </p><p id="""">‼️ Les signes que vous êtes prêt à passer du notebook au code source incluent :</p><ul id=""""><li id="""">la duplication de notebooks pour en créer de nouveaux</li><li id=""""> le copier-coller de fonctions entre notebooks</li><li id=""""> la création de classes orientées objet dans des notebooks.</li></ul><p id="""">💡Tips : Exécuter cela dans la première cellule du notebook </p><p id=""""><strong id="""">	%load_ext autoreload<br>	%autoreload 2</strong></p><p id="""">Pour que les changements dans votre code source soient pris en compte dans votre notebook directement.</p><p id="""">‍</p><h3 id=""""><strong id="""">5. Ne pas négliger l’organisation des modèles </strong></h3><p id="""">Documenter les expériences de modélisation est essentiel pour assurer la reproductibilité, l'apprentissage continu et l'amélioration. Vous devez mettre en place des procédures de documentation permettant d'identifier au minimum </p><ul id=""""><li id="""">la provenance des données</li><li id="""">la version du code utilisée </li><li id="""">les métriques de performance.</li></ul><p id="""">Vous pouvez même utiliser des outils plus avancés tel que <strong id="""">MLFlow.</strong></p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/67095b0bde3d693999333aa3_67095adaaf104015f2866282_Capture%2520d%25E2%2580%2599e%25CC%2581cran%25202024-10-11%2520a%25CC%2580%252019.05.02.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure>";;Datascience;ML;Data;;;;;;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89e9f1e38e61d230448af_replicate-prediction-p7t3ndaey9rm60chzjn9cjtp5w.webp;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6709559c6de55cd0b2d4861d_ccds_article_hero.jpeg;Cet article explore l'importance d'une structure solide dans les projets de machine learning et présente CookieCutter, un outil puissant pour organiser vos travaux de data science. Vous y découvrirez les principes clés d'une analyse de données reproductible, notamment l'approche DAG (graphe orienté acyclique), l'importance des données brutes immuables, et l'utilisation judicieuse des notebooks et du code source. L'article souligne également l'importance de la documentation des expériences de modélisation et offre des conseils pratiques pour améliorer la qualité et la reproductibilité de vos projets de machine learning. Une lecture essentielle pour tout data scientist cherchant à professionnaliser sa démarche et à optimiser ses flux de travail.;Structurer un Projet de ML | CookieCutter et best practices;Structurez vos projets de machine learning avec CookieCutter. Apprenez les bests pratiques pour une analyse de données reproductible et de qualité.
dependency_hells;dependency-hell;66ddd0fdfa9ffe66693562eb;66cf477ebc1fcf6548c64d6b;671e2ce5b80f279d61b57738;Sun Oct 27 2024 12:07:01 GMT+0000 (Coordinated Universal Time);Sun Oct 27 2024 15:50:22 GMT+0000 (Coordinated Universal Time);Sun Dec 01 2024 12:09:33 GMT+0000 (Coordinated Universal Time);Dependencys Hell : Comment l'Injection de Dépendances Peut Sauver Vos Tests;erraji badr;Sun Oct 27 2024 00:00:00 GMT+0000 (Coordinated Universal Time);10 min;false;false;false;;"<h2 id="""">Introduction</h2><p id="""">En tant que développeur, vous avez probablement déjà rencontré le fameux ""dependency hell"" - cette situation où votre code devient un véritable cauchemar à tester à cause de multiples dépendances entremêlées. Dans cet article, nous allons explorer comment l'injection de dépendances peut transformer ce chaos en une architecture propre et maintenable.</p><p id="""">‍</p><h2 id="""">Qu'est-ce que l'Injection de Dépendances ?</h2><p id="""">L'injection de dépendances est un Design Pattern qui permet de découpler les composants de votre application. Au lieu de créer leurs propres dépendances, les classes les reçoivent de l'extérieur. Ce simple changement apporte des bénéfices considérables pour la qualité de votre code.</p><p id="""">‍</p><h2 id="""">Le Problème : Le Dependency Hell Illustré</h2><p id="""">Voici un exemple classique de code sans injection de dépendances :</p><p id="""">‍</p><pre></pre><p id="""">‍</p><p id="""">Les tests deviennent rapidement complexes et difficiles à maintenir :</p><p id="""">‍</p><pre></pre><p id="""">‍</p><h3 id="""">La Solution : L'Injection de Dépendances</h3><p id="""">Voici comment le même code peut être réécrit avec l'injection de dépendances :</p><p id="""">‍</p><pre></pre><p id="""">‍</p><p id="""">Les tests deviennent beaucoup plus simples et lisibles :</p><p id="""">‍</p><pre></pre><p id="""">‍</p><h2 id="""">Les Avantages de l'Injection de Dépendances</h2><h3 id="""">1. Tests Simplifiés</h3><ul id=""""><li id="""">Plus besoin de patches complexes</li><li id="""">Tests plus lisibles et maintenables</li><li id="""">Moins de code de configuration</li></ul><h3 id="""">2. Flexibilité Accrue</h3><ul id=""""><li id="""">Changement facile d'implémentation</li><li id="""">Adaptation rapide aux nouveaux besoins</li><li id="""">Meilleure réutilisation du code</li></ul><h3 id="""">3. Maintenance Facilitée</h3><ul id=""""><li id="""">Responsabilités clairement définies</li><li id="""">Code plus modulaire</li><li id="""">Debugging simplifié</li></ul><h3 id="""">4. Isolation Parfaite</h3><ul id=""""><li id="""">Tests unitaires véritablement isolés</li><li id="""">Pas de dépendance aux services externes</li><li id="""">Meilleure fiabilité des tests</li></ul><h2 id="""">Quand Utiliser l'Injection de Dépendances ?</h2><p id="""">L'injection de dépendances n'est pas toujours nécessaire. Voici quelques indicateurs pour savoir quand l'adopter :</p><ul id=""""><li id="""">Composants complexes avec plusieurs dépendances</li><li id="""">Code nécessitant des tests approfondis</li><li id="""">Services susceptibles de changer d'implémentation</li><li id="""">Besoin de flexibilité dans l'architecture</li></ul><h2 id="""">Bonnes Pratiques</h2><ol id=""""><li id="""">Commencez simple : n'injectez que les dépendances nécessaires</li><li id="""">Utilisez des interfaces pour définir les contrats</li><li id="""">Considérez un container d'injection de dépendances pour les grands projets</li><li id="""">Documentez les dépendances requises</li></ol><h2 id="""">Conclusion</h2><p id="""">L'injection de dépendances est un pattern puissant qui, bien utilisé, peut considérablement améliorer la qualité de votre code. Elle simplifie les tests, rend le code plus flexible et facilite la maintenance. Même si elle peut sembler complexe au début, les bénéfices à long terme en font un outil indispensable dans l'arsenal du développeur moderne.</p><h2 id="""">Pour Aller Plus Loin</h2><ul id=""""><li id="""">Explorez les frameworks d'injection de dépendances</li><li id="""">Apprenez les design patterns associés</li><li id="""">Pratiquez sur des projets existants</li><li id="""">Partagez vos expériences avec la communauté</li></ul><p id="""">N'oubliez pas : la simplicité est la clé. Commencez petit et évoluez progressivement vers des solutions plus sophistiquées selon vos besoins.</p><p id="""">‍</p><p id="""">‍</p><p id="""">Mots-clés : injection de dépendances, tests unitaires, architecture logicielle, design patterns, qualité du code, développement Python, bonnes pratiques, maintenance du code</p>";;code quality;python;test;TDD;;;;;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89e9f1e38e61d230448af_replicate-prediction-p7t3ndaey9rm60chzjn9cjtp5w.webp;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/671e2c9bff78b91a83807010_nine-circles-of-dependency-hell.jpg;"L'injection de dépendances est une technique de programmation qui permet de résoudre le problème du ""dependency hell"" - une situation où le code devient difficile à tester à cause de dépendances trop nombreuses et étroitement couplées. À travers un exemple concret en Python, l'article montre comment transformer un code difficile à tester en une architecture propre et maintenable.";Comment Simplifier vos Tests avec l'Injection de Dépendances;Découvrez comment l'injection de dépendances peut transformer votre code spaghetti en une architecture maintenable. Guide pratique avec exemples python
testing-locally-glue-jobs;testing-locally-glue-jobs;66ddd0fdfa9ffe66693562eb;66cf477ebc1fcf6548c64d6b;6702a7d2ed5bb8f32841050a;Sun Oct 06 2024 15:08:02 GMT+0000 (Coordinated Universal Time);Sun Oct 06 2024 16:27:39 GMT+0000 (Coordinated Universal Time);Sun Oct 06 2024 16:28:43 GMT+0000 (Coordinated Universal Time);AWS Glue Job - Pourquoi & comment tester en local ?;Erraji Badr;Mon Sep 30 2024 00:00:00 GMT+0000 (Coordinated Universal Time);7min;false;false;false;"<h2 id=""""><a href=""#titre_1"" id=""""><strong id="""">I&nbsp; - Pourquoi tester en local ?</strong></a><strong id=""""> </strong></h2><p id="""">‍</p><h2 id=""""><a href=""#titre_2"" id=""""><strong id="""">II - Comment tester en local ?</strong></a></h2><p id="""">‍</p><h2 id=""""><a href=""#titre_3"">III- Bonus &amp; <strong id="""">have fun =) </strong></a></h2><p id="""">‍</p>";"<p id="""">AWS Glue est un service serverless phare d’AWS pour l’intégration de données. Pour les data engineers spécialisés en big data, sa magie réside principalement dans le fait qu’il nous permet de nous concentrer entièrement sur Spark et son optimisation, tout en déléguant à AWS la gestion de l'infrastructure avec un paramétrage minimal pour commencer. Bien sûr, il est possible de peaufiner la configuration pour les jobs les plus complexes.</p><p id="""">‍</p><p id="""">‍</p><div data-rt-embed-type='true'><a name=""titre_1""></a></div><h2 id=""""><strong id="""">I&nbsp; - Pourquoi tester en local ? </strong></h2><p id="""">‍</p><p id="""">Le serverless, bien qu'extrêmement pratique pour le calcul distribué, peut également rendre la phase d'implémentation et de test complexe. Chaque modification implique de lancer le job, d'attendre que le cluster soit prêt, et de découvrir parfois que le job échoue dès le début à cause d'une policy IAM manquante, d'une erreur sur les colonnes d'un DataFrame, ou d'autres problèmes similaires.</p><p id="""">‍</p><p id=""""><strong id="""">Qui a déjà réussi à lancer ses scripts dès le début sans aucun bug ?</strong> Si vous avez le secret, faites-le-moi savoir, je suis très intéressé !</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6702aae5e24c2ee00d8b53aa_6702a951a9ba3ae4bc53b2bb_glue_fails.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Le coût de l’itération de développement est élevé :</p><ul id=""""><li id=""""><strong id="""">5 à 15 minutes</strong> perdues pour déployer le script Glue et lancer le job.</li><li id=""""><strong id="""">Coût des DPU</strong> pour toutes les minutes où le cluster a tourné avant d’échouer.</li><li id=""""><strong id="""">Temps de débogage</strong> sans possibilité de déboguer directement l’environnement Glue.</li><li id=""""><strong id="""">Pour le sessions interactives,</strong> Grosse économie de DPU&nbsp;pendant les devs.</li></ul><p id="""">Tester en local devient alors une nécessité pour accélérer le développement et réduire les coûts en ressources humaines et matérielles. </p><p id="""">‍</p><p id="""">En bonus, cela permet d’implémenter des tests unitaires, une pratique essentielle pour un développement serein.</p><p id="""">‍</p><p id="""">‍</p><div data-rt-embed-type='true'><a name=""titre_2""></a></div><h2 id=""""><strong id="""">II - Comment tester en local ?</strong></h2><p id="""">‍</p><p id="""">Pour réussir à tester en local, voici les trois étapes principales :</p><ol id=""""><li id=""""><strong id="""">Puller l’image Docker Glue à la version qui vous intéresse</strong> (à ce jour, la version 4.0).</li><li id=""""><strong id="""">Lancer et attacher le conteneur à votre IDE</strong> (Pycharm). Pour Visual Studio Code, voir la documentation existante sur mon <a href=""https://github.com/errajibadr/DataEngineeringUnboxed/blob/main/aws/glue_local_VSCode/how_to_run_glue_with_vsCode.md"" id="""">github.</a><ul id=""""><li id=""""><strong id="""">Option bonus</strong> : L’attacher à un notebook pour une exploration plus interactive des données.<strong id="""">‍</strong></li></ul></li><li id=""""><strong id=""""> Lancer le débogage, les tests unitaires, ou utiliser le notebook</strong> pour découvrir la donnée et tester pas à pas.</li></ol><p id="""">‍</p><p id="""">‍</p><h3 id=""""><strong id="""">1 - Puller l’image Glue sur Docker</strong></h3><p id="""">‍</p><p id="""">Pour Glue version <strong id="""">2.0</strong>, <strong id="""">3.0 </strong>ou <strong id="""">4.0</strong>, utilisez la commande suivante dans le terminal :</p><p id="""">‍</p><pre></pre><p id="""">‍</p><p id="""">‍</p><h3 id=""""><strong id="""">2 - Configurer Pycharm et attacher l’image</strong></h3><p id="""">‍</p><p id=""""><strong id="""">Dans cette section, nous allons voir comment exécuter notre code dans l’IDE (Pycharm) pour qu’il se lance avec l’environnement Glue de l’image Docker.</strong></p><p id="""">Une fois votre projet pyspark ouvert. ( et si vous n’en avez pas, vous pouvez simplement créer un projet et rajouter ce fichier glue_script.py qu’on pourra tester tout à l’heure. ) </p><p id="""">‍</p><p id="""">‍</p><h5 id="""">A - Configurer l’interpréteur Python :</h5><ul id=""""><li id="""">Ouvrez votre projet PySpark dans Pycharm. (Si vous n’avez pas de projet, vous pouvez en créer un et ajouter un fichier glue_script.py pour tester.) </li></ul><ul id=""""><li id=""""><strong id="""">Configurer l’interpréteur Python</strong> :</li></ul><ul id=""""><li id="""">Allez dans <strong id="""">File &gt; Settings</strong> et recherchez ""Python Interpreter"". Cliquez sur <strong id="""">Add Interpreter </strong>et bien sélectionner<strong id=""""> - on Docker - </strong></li></ul><ul id=""""><li id="""">Choisissez <strong id="""">Docker</strong> et <strong id="""">Pull or use existing</strong> et dans<strong id=""""> image tag </strong>utilisez l’image téléchargée plus haut (amazon/aws-glue_libs:glue_libs_4.0.0.0_image_01).</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6702aae5e24c2ee00d8b53a1_6702a9d659c5c337df5188cc_docker_interpreter.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><ul id=""""><li id="""">Dans l'étape 3 : Assurez-vous que l’interpréteur Python détecté est /usr/local/bin/python3.</li></ul><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6702aae5e24c2ee00d8b53ca_6702aaa59cd01719052b6a39_docker_interperter_02.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><ul id=""""><li id="""">Cliquez sur <strong id="""">Create</strong>.</li></ul><p id="""">‍</p><h5 id="""">B - Edit configuration script </h5><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6702aae5e24c2ee00d8b53a4_6702a9a7000d95f005c22ffd_edit_conf.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id=""""> Dans la conf, nous allons principalement modifier les <strong id="""">variables d’environnement</strong> ainsi que <strong id="""">docker container settings</strong>.</p><p id="""">‍</p><h6 id=""""><strong id="""">1 - Python Path : </strong></h6><p id="""">‍</p><p id="""">Maintenant qu’on a configuré l’interpréteur, il faut faire attention à bien spécifier à pycharm où est-ce qu’il va trouver les fichiers sources python utilisés dans le conteneur Docker. </p><p id="""">On devra ajouter une variable d’environnement <strong id="""">PYTHONPATH </strong></p><p id=""""><strong id="""">Pour glue 4.0, on devra la compléter avec </strong></p><p id=""""><strong id="""">/home/glue_user/aws-glue-libs/PyGlue.zip:/home/glue_user/spark/python/lib/py4j-0.10.9-src.zip:/home/glue_user/spark/python/</strong></p><p id="""">‍</p><p id=""""><strong id="""">Pour les autres versions, il faut aller voir le build docker de l’image </strong></p><p id=""""><a href=""https://hub.docker.com/r/amazon/aws-glue-libs/tags"" id="""">https://hub.docker.com/r/amazon/aws-glue-libs/tags</a></p><p id="""">‍</p><p id=""""><strong id="""">Choisir la version qui nous intéresse et récupérer la variable Python Path comme ci-dessous.</strong></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6702ac2a9de9795a1aa5d675_6702ab3bab354a1ef78e3b35_Capture%2520d%25E2%2580%2599e%25CC%2581cran%25202024-10-06%2520a%25CC%2580%252017.22.09.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍</p><h6 id=""""><strong id="""">2 - Docker Container Settings :</strong></h6><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6702aae5e24c2ee00d8b53a7_6702aa5a131d41ce93108f5c_pycharm_docker_config.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Concrètement<strong id=""""> </strong>c’est là où on va faire la configuration de run du <strong id="""">Docker</strong>. </p><p id="""">Les fameux paramètre -v ou -p par ex. lorsqu’on fait un Docker Run sur terminal.</p><p id=""""> </p><p id="""">Petit rappel : </p><p id="""">v pour volume mounting, p port binding par ex: &nbsp;-v local_path:docker_path -p 8080:80 etc.</p><p id="""">Ici pour l’intégration avec AWS, il ne faudra pas oublier d’inclure les Credentials AWS dans Volume bindings.</p><ul id=""""><li id=""""><strong id="""">Volume bindings</strong> : Host path: ~/.aws, Container path: /root/.aws</li></ul><p id="""">N’oublions pas de rajouter les variables d’environnement dans le &nbsp;<strong id="""">AWS_PROFILE</strong> et <strong id="""">AWS_REGION avec </strong>vos paramètres.</p><p id="""">‍</p><p id="""">‍</p><div data-rt-embed-type='true'><a name=""titre_3""></a></div><h4 id=""""><strong id="""">C - Lancer le script &amp; have fun =) </strong></h4><p id="""">‍</p><p id="""">À partir de là, vous pouvez lancer le script ou le déboguer exactement comme un script normal dans Pycharm. L’exécution sera légèrement plus longue car le conteneur doit se lancer et charger Spark avant que le code ne s’exécute, mais cela ne prend que quelques secondes, bien moins que de devoir déployer le code sur Glue et tester directement sur le job.</p><p id="""">‍<strong id="""">‍</strong></p><h4 id=""""><strong id=""""> Bonus: </strong></h4><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6702aae5e24c2ee00d8b53b6_6702aa6e1cd620f266247328_module_linting.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍<br></p><p id="""">Les modules et objets Glue de votre projet ou script peuvent ne pas être reconnus et être soulignés en rouge dans Pycharm. Cela est dû au fait que Pycharm ne les reconnaît pas, car votre interpréteur Python est dans l’image Docker.</p><p id="""">‍</p><p id="""">Une astuce simple est de cloner le repo sur Github : <a href=""https://github.com/awslabs/aws-glue-libs"" id="""">https://github.com/awslabs/aws-glue-libs</a></p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6702ac2a9de9795a1aa5d64b_6702ab93000d95f005c43757_Capture%2520d%25E2%2580%2599e%25CC%2581cran%25202024-10-06%2520a%25CC%2580%252017.23.52.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Clonez la branche qui vous intéresse. La branche master est pour la dernière version GLUE dispo. Ici c’est la 4.0.</p><pre></pre><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6702aae5e24c2ee00d8b53b3_6702aa96db95efc3aad45875_project_structure.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Ensuite, dans Pycharm, allez dans <strong id="""">File &gt; Settings &gt; Project Structure &gt; Add Content Roots </strong>et ajoutez le chemin local où vous avez cloné aws-glue-libs.</p><p id="""">‍</p><p id="""">‍</p><p id=""""><strong id="""">Conclusion</strong></p><p id="""">Le cloud computing a révolutionné l’industrie tech, notamment celle de la data, en permettant de gérer de grandes quantités de stockage et de calcul sans avoir besoin de lourdes infrastructures physiques. Le calcul distribué est au cœur de ces architectures à grande échelle, mais il nécessite une expertise poussée pour être géré efficacement.</p><p id="""">Le serverless, en ajoutant une couche d’abstraction, permet de se concentrer directement sur le côté fonctionnel sans le moindre overhead opérationnel. Il y a bien des serveurs derrière, mais vous n’avez pas besoin de les gérer ou de les administrer. Vous pouvez vous concentrer à 100 % sur la fonctionnalité.</p><p id="""">Tester vos jobs AWS Glue en local vous permet de gagner du temps et de l'argent tout en vous assurant que votre code est prêt pour la production, réduisant ainsi les risques de bugs après déploiement.</p><p id="""">‍</p><p id="""">‍</p>";;big data;spark;aws;glue;serverless;;;;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/66e89e9f1e38e61d230448af_replicate-prediction-p7t3ndaey9rm60chzjn9cjtp5w.webp;https://cdn.prod.website-files.com/66cf477ebc1fcf6548c64d6c/6702b81e131d41ce931e4f54_replicate-prediction-em4gkwm11drm40cjcadrv4yxng.webp;Ce guide pratique explique comment tester localement les jobs AWS Glue, un service serverless d'intégration de données. L'article souligne l'importance du test local pour accélérer le développement, réduire les coûts et faciliter le débogage. Il détaille ensuite une méthode en trois étapes pour configurer un environnement de test local. Ce tutoriel vise à optimiser le processus de développement des jobs AWS Glue, permettant aux data engineers de tester efficacement leur code avant le déploiement en production.;AWS Glue Jobs (Pyspark) - Pourquoi & comment tester en local;Guide pratique pour tester localement les jobs AWS Glue. Optimisez votre développement, réduisez les coûts et facilitez le débogage avec Docker et PyCharm.